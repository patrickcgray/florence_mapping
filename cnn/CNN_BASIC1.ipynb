{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN Framework for Landsat Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from itertools import product\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open(\"/deep_data/data/land_cover_data/landcover_reproject.tif\")\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "tiles = [\"028011\", \"028012\"]\n",
    "\n",
    "landsat_dataset = list(i for i in range(len(tiles)))\n",
    "count = 0\n",
    "for tile in tiles:\n",
    "    landsat_dataset[count] = rasterio.open(\"/deep_data/data/combined/combined\" + tile + \".tif\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-59c69bf46f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# What is the raster's projection?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlandsat_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m \u001b[0;31m# 4326\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image projection:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_dataset[1].crs # 4326\n",
    "print(\"1\")\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "\n",
    "\n",
    "image_proj = landsat_dataset[0].crs # 4326\n",
    "print(\"0\")\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "\n",
    "\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator and Prep Fcns\n",
    "\n",
    "This is a typical Keras generator that I've written to allow it to ingest a set of random pixel locations so we can randomly sample throughout the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, band_count))\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile)\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "                middle_pixel_r = r + np.ceil(tile_width/2)\n",
    "                middle_pixel_c = c + np.ceil(tile_height/2)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(middle_pixel_r, middle_pixel_c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of raster datasets and randomly samples `train_count` and `val_count` random pixels from each dataset.\n",
    "\n",
    "It doesn't sample within tile_size / 2 of the edge in order to avoid missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a randomly selected number of pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    total_count = train_count + val_count\n",
    "    for index, image_dataset in enumerate(image_datasets):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(product(rows, columns)), total_count)\n",
    "        \n",
    "        dataset_index = [index] * total_count\n",
    "        \n",
    "        dataset_pixels = list(zip(points, dataset_index))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count]\n",
    "        val_pixels += dataset_pixels[train_count:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the generator and data prep functions\n",
    "\n",
    "Let's make sure all this data prep actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and validation pixel locations\n",
    "train_px, val_px = make_pixel_locations(landsat_dataset, 100, 20, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# print out some image and label batches and check out their shapes\n",
    "im_batch = None\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_generator(landsat_dataset, label_dataset, 11, 11, train_px, 2):\n",
    "    if count > 3:\n",
    "        break\n",
    "    print('Image')\n",
    "    print(im.shape)\n",
    "    print('Label')\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect an image patch\n",
    "\n",
    "While it shouldn't necessarily be recognizable it should look like it has data in it and that it varies somewhat from pixel to pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5ce0fa6390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(18, 5)) \n",
    "\n",
    "axs.imshow(im_batch[0,:,:,1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to the CNN Development!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 7)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "num_classes = len(np.unique(label_image))\n",
    "epochs = 100\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 11, 11\n",
    "img_bands = landsat_dataset[0].count\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the model\n",
    "\n",
    "This is just a simple CNN model but it should be able to perform well above random when predicting landcover types if everything is correct thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 11)          704       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 22)          2200      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 5, 44)          8756      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 128)         50816     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 44)                5676      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 23)                1035      \n",
      "=================================================================\n",
      "Total params: 69,187\n",
      "Trainable params: 69,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(11, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(22, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(44, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(44, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the train/validation pixel locations to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pixel_locations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-931c20dc3dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_px, val_px = make_pixel_locations(image_datasets=landsat_dataset, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                        train_count=10000, val_count=300, tile_size=11)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_pixel_locations' is not defined"
     ]
    }
   ],
   "source": [
    "train_px, val_px = make_pixel_locations(image_datasets=landsat_dataset, \n",
    "                                       train_count=10000, val_count=300, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up the remaining model hyperparameters and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "756/800 [===========================>..] - ETA: 1s - loss: 1.7661 - acc: 0.4643"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b39213541dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_px\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandsat_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=len(val_px) // batch_size)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator(landsat_dataset, label_dataset, 11, 11, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_dataset, label_dataset, 11, 11, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's evaluate the Model\n",
    "\n",
    "We'll just generate 500 test pixels to evaluate it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'landsat_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1caaadcb9f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_px\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_px\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pixel_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlandsat_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtile_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'landsat_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_px, val_px = make_pixel_locations([landsat_dataset], train_count=500, val_count=0, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a built in evaluate_generator function and because we told it above to use accuracy as a metric this function automatically outputs categorical accuracy which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 8s 376ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9832591772079469, 0.3760000020265579]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this simple model we're getting 37% accuracy across 23 classes which is well above the random accuracy which would be around 4% (aka 1/23). That means we're in business!\n",
    "\n",
    "### Evaluating the model ourselves\n",
    "\n",
    "If we wanted to run this evaluation and take a look at specific predictions and labels we can do that below (albeit more inefficiently) just to get an intuitive understanding of what is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 7s 373ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(test_px):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 23)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 14,  5, 13, 13, 20,  8, 21, 13, 12])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = np.zeros(pred_index.shape)\n",
    "correct_predictions[label_index == pred_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1.])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.376"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(correct_predictions) / len(test_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.5005555e-05, 2.5345067e-05, 6.9519803e-03, 9.5544951e-03,\n",
       "        1.7500592e-02, 4.5062839e-03, 7.9424324e-05, 5.6560391e-05,\n",
       "        4.5279139e-03, 8.9906462e-05, 8.1206148e-04, 4.5664867e-05,\n",
       "        5.2689546e-04, 2.0000549e-02, 8.8348606e-04, 2.6076639e-02,\n",
       "        3.8386403e-05, 7.5204247e-05, 5.2162164e-01, 3.7951332e-02,\n",
       "        6.4366475e-02, 2.8416619e-01, 6.7970672e-05],\n",
       "       [9.6968334e-04, 6.7671924e-04, 1.0775626e-02, 1.2592370e-02,\n",
       "        1.2026070e-02, 1.2465723e-02, 3.3110134e-02, 1.2139344e-03,\n",
       "        2.5713392e-02, 2.2136976e-03, 3.7321962e-02, 2.8844578e-03,\n",
       "        6.2341351e-02, 1.9656718e-01, 1.5323413e-02, 1.5057945e-01,\n",
       "        4.1870007e-04, 6.7505526e-04, 8.9561611e-02, 1.7911386e-02,\n",
       "        2.0733811e-02, 2.9347154e-01, 4.5277303e-04],\n",
       "       [2.0889305e-04, 1.2599016e-04, 9.1587409e-04, 8.8419262e-03,\n",
       "        5.2371044e-02, 3.6170930e-02, 3.1249449e-01, 5.3113926e-04,\n",
       "        1.3841026e-01, 6.5567750e-03, 1.9548453e-01, 3.2652885e-02,\n",
       "        9.5884621e-02, 5.9320562e-02, 4.4410080e-02, 5.9092906e-03,\n",
       "        1.7554613e-04, 1.5845097e-04, 1.2757443e-03, 1.7050681e-04,\n",
       "        6.1450154e-03, 1.6437772e-03, 1.4168759e-04]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10,  6, 13, 13, 21, 13, 13,  6,\n",
       "       13,  6, 21])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0,  6,  0,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 43,  0,  0,  4,  0,  0,  8,  0,  0,  0,  0,  3],\n",
       "       [ 0,  3,  0,  9,  0,  0,  6,  0,  0,  3,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0, 25,  2,  0,  0,  0,  0, 32,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0, 11,  0,  0,  0,  0,  2],\n",
       "       [ 0,  6,  0,  8,  0,  0,  9,  0,  0, 25,  0,  0,  3,  0,  0],\n",
       "       [ 0,  6,  0, 26,  0,  0, 14,  0,  0, 86,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  5,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  3,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  0,  0,  3,  0,  0,  2,  0, 47]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(label_index, pred_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
