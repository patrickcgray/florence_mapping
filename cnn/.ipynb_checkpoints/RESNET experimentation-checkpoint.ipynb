{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dict((\n",
    "(0,  'Background'),\n",
    "(1, 'Unclassified'),\n",
    "(2, 'High Intensity Developed'),\n",
    "(3, 'Medium Intensity Developed'),\n",
    "(4, 'Low Intensity Developed'),\n",
    "(5, 'Open Space Developed'),\n",
    "(6, 'Cultivated Land'),\n",
    "(7, 'Pasture/Hay'),\n",
    "(8, 'Grassland'),\n",
    "(9, 'Deciduous Forest'),\n",
    "(10, 'Evergreen Forest'),\n",
    "(11, 'Mixed Forest'),\n",
    "(12, 'Scrub/Shrub'),\n",
    "(13, 'Palustrine Forested Wetland'),\n",
    "(14, 'Palustrine Scrub/Shrub Wetland'),\n",
    "(15, 'Palustrine Emergent Wetland'),\n",
    "(16, 'Estuarine Forested Wetland'),\n",
    "(17, 'Estuarine Scrub/Shrub Wetland'),\n",
    "(18, 'Estuarine Emergent Wetland'),\n",
    "(19, 'Unconsolidated Shore'),\n",
    "(20, 'Bare Land'),\n",
    "(21, 'Water'),\n",
    "(22, 'Palustrine Aquatic Bed'),\n",
    "(23, 'Estuarine Aquatic Bed'),\n",
    "(24, 'Tundra'),\n",
    "(25, 'Snow/Ice')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import classifier_utilities as cu\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "import os\n",
    "import resnet as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open('/deep_data/landcover_reproject.tif')\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "l8_image_paths = [\n",
    "    '/deep_data/processed_landsat/LC08_CU_027012_20170907_20181121_C01_V01_SR_combined.tif',\n",
    "    '/deep_data/processed_landsat/LC08_CU_028011_20170907_20181130_C01_V01_SR_combined.tif',  \n",
    "    '/deep_data/processed_landsat/LC08_CU_028012_20171002_20171019_C01_V01_SR_combined.tif',\n",
    "    '/deep_data/processed_landsat/LC08_CU_028012_20171103_20190429_C01_V01_SR_combined.tif',\n",
    "    '/deep_data/processed_landsat/LC08_CU_029011_20171018_20190429_C01_V01_SR_combined.tif'\n",
    "]\n",
    "\n",
    "s1_image_paths = [\n",
    "    '/deep_data/sentinel_sar/LC08_CU_027012_20170907_20181121_C01_V01_SR_combined/aligned-LC08_CU_027012_20170907_20181121_C01_V01_SR_combined_SAR.tif',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_028011_20170907_20181130_C01_V01_SR_combined/aligned-LC08_CU_028011_20170907_20181130_C01_V01_SR_combined_SAR.tif',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_028012_20171002_20171019_C01_V01_SR_combined/aligned-LC08_CU_028012_20171002_20171019_C01_V01_SR_combined_SAR.tif',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_028012_20171103_20190429_C01_V01_SR_combined/aligned-LC08_CU_028012_20171103_20190429_C01_V01_SR_combined_SAR.tif',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_029011_20171018_20190429_C01_V01_SR_combined/aligned-LC08_CU_029011_20171018_20190429_C01_V01_SR_combined_SAR.tif',\n",
    "]\n",
    "\n",
    "dem_image_paths = [\n",
    "    '/deep_data/sentinel_sar/LC08_CU_027012_20170907_20181121_C01_V01_SR_combined_dem/aligned-wms_DEM_EPSG4326_-79.69001_33.95762_-77.7672_35.51886__4500X4631_ShowLogo_False_tiff_depth=32f.tiff',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_028011_20170907_20181130_C01_V01_SR_combined_dem/aligned-wms_DEM_EPSG4326_-77.7672_35.00779_-75.79042_36.58923__4500X4262_ShowLogo_False_tiff_depth=32f.tiff',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_028012_20171002_20171019_C01_V01_SR_combined_dem/aligned-wms_DEM_EPSG4326_-79.69001_33.95762_-77.7672_35.51886__4500X4631_ShowLogo_False_tiff_depth=32f.tiff',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_028012_20171103_20190429_C01_V01_SR_combined_dem/aligned-wms_DEM_EPSG4326_-78.07896_33.69485_-76.14021_35.27466__4500X4248_ShowLogo_False_tiff_depth=32f.tiff',\n",
    "    '/deep_data/sentinel_sar/LC08_CU_029011_20171018_20190429_C01_V01_SR_combined_dem/aligned-wms_DEM_EPSG4326_-76.14021_34.71847_-74.14865_36.318__4500X4408_ShowLogo_False_tiff_depth=32f.tiff',\n",
    "]\n",
    "\n",
    "\n",
    "landsat_datasets = []\n",
    "for fp in l8_image_paths:\n",
    "    landsat_datasets.append(rasterio.open(fp))\n",
    "    \n",
    "sentinel_datasets = []\n",
    "for fp in s1_image_paths:\n",
    "    sentinel_datasets.append(rasterio.open(fp))\n",
    "    \n",
    "dem_datasets = []\n",
    "for fp in dem_image_paths:\n",
    "    dem_datasets.append(rasterio.open(fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:03<00:00, 94.99s/it]\n"
     ]
    }
   ],
   "source": [
    "train_pixels = cu.gen_balanced_pixel_locations(image_datasets=landsat_datasets, train_count=500000, \n",
    "                                            label_dataset=label_dataset, merge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valratio = 0.9\n",
    "val_testratio = 0.7\n",
    "train_px = train_pixels[:int(len(train_pixels)*train_valratio)]\n",
    "val_pixels = train_pixels[int(len(train_pixels)*train_valratio):]\n",
    "val_px = val_pixels[:int(len(val_pixels)*val_testratio)]\n",
    "test_px = val_pixels[int(len(val_pixels)*val_testratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359252\n",
      "27941\n",
      "11976\n"
     ]
    }
   ],
   "source": [
    "print(len(train_px))\n",
    "print(len(val_px))\n",
    "print(len(test_px))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "epochs = 200\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# input image dimensions\n",
    "tile_side = 64\n",
    "img_rows, img_cols = tile_side, tile_side\n",
    "img_bands = landsat_datasets[0].count + sentinel_datasets[0].count + dem_datasets[0].count - 1\n",
    "\n",
    "input_shape = (img_bands, img_rows, img_cols)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rs.ResnetBuilder.build_resnet_101(input_shape,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14370/14370 [==============================] - 6174s 430ms/step - loss: 12.7084 - acc: 0.2706 - val_loss: 12.4563 - val_acc: 0.3337\n",
      "Epoch 2/200\n",
      "14370/14370 [==============================] - 6099s 424ms/step - loss: 12.3713 - acc: 0.3452 - val_loss: 12.2756 - val_acc: 0.3617\n",
      "Epoch 3/200\n",
      "14370/14370 [==============================] - 5938s 413ms/step - loss: 12.1953 - acc: 0.3841 - val_loss: 12.1618 - val_acc: 0.3794\n",
      "Epoch 4/200\n",
      "14370/14370 [==============================] - 5862s 408ms/step - loss: 12.0528 - acc: 0.4159 - val_loss: 12.0775 - val_acc: 0.3893\n",
      "Epoch 5/200\n",
      "14370/14370 [==============================] - 5862s 408ms/step - loss: 11.9212 - acc: 0.4439 - val_loss: 11.9952 - val_acc: 0.3984\n",
      "Epoch 6/200\n",
      "14370/14370 [==============================] - 5863s 408ms/step - loss: 11.7913 - acc: 0.4742 - val_loss: 11.9369 - val_acc: 0.4006\n",
      "Epoch 7/200\n",
      "14370/14370 [==============================] - 5862s 408ms/step - loss: 11.6593 - acc: 0.5049 - val_loss: 11.8724 - val_acc: 0.4088\n",
      "Epoch 8/200\n",
      "14370/14370 [==============================] - 5860s 408ms/step - loss: 11.5216 - acc: 0.5389 - val_loss: 11.8081 - val_acc: 0.4119\n",
      "Epoch 9/200\n",
      "14370/14370 [==============================] - 5861s 408ms/step - loss: 11.3754 - acc: 0.5759 - val_loss: 11.7693 - val_acc: 0.4097\n",
      "Epoch 10/200\n",
      "14370/14370 [==============================] - 5863s 408ms/step - loss: 11.2207 - acc: 0.6167 - val_loss: 11.7356 - val_acc: 0.4116\n",
      "Epoch 11/200\n",
      "14370/14370 [==============================] - 5863s 408ms/step - loss: 11.0535 - acc: 0.6614 - val_loss: 11.7041 - val_acc: 0.4126\n",
      "Epoch 12/200\n",
      "14370/14370 [==============================] - 5861s 408ms/step - loss: 10.8789 - acc: 0.7092 - val_loss: 11.7115 - val_acc: 0.4126\n",
      "Epoch 13/200\n",
      "14370/14370 [==============================] - 5861s 408ms/step - loss: 10.6947 - acc: 0.7597 - val_loss: 11.7236 - val_acc: 0.4114\n",
      "Epoch 14/200\n",
      "14370/14370 [==============================] - 5861s 408ms/step - loss: 10.5047 - acc: 0.8101 - val_loss: 11.7322 - val_acc: 0.4133\n",
      "Epoch 15/200\n",
      " 3416/14370 [======>.......................] - ETA: 1:13:10 - loss: 10.3926 - acc: 0.8384"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-aa8691f36b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_px\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandsat_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentinel_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdem_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_side\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_side\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=len(val_px) // batch_size)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=cu.tile_generator(landsat_datasets, sentinel_datasets, dem_datasets, label_dataset, tile_side, tile_side, train_px, batch_size, merge=False), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=cu.tile_generator(landsat_datasets, sentinel_datasets, dem_datasets, label_dataset, tile_side, tile_side, val_px, batch_size, merge=False),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - 47s 98ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.574935222219574, 0.31849686883872136]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator=cu.tile_generator(landsat_datasets, sentinel_datasets, dem_datasets, label_dataset, tile_side, tile_side, test_px, batch_size,merge=True), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/479 [=====================>........] - ETA: 11s"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(generator=cu.tile_generator(landsat_datasets, sentinel_datasets, dem_datasets, \n",
    "        label_dataset, tile_side, tile_side, test_px, batch_size, merge=True), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)\n",
    "\n",
    "eval_generator = cu.tile_generator(landsat_datasets, sentinel_datasets, dem_datasets, \n",
    "                                label_dataset, tile_side, tile_side, test_px, batch_size=1, merge=True)\n",
    "\n",
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(labels):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1\n",
    "    \n",
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "cu.plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "cu.plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11975, 26)\n",
      "[13  5  9 ...  6  4 21]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
