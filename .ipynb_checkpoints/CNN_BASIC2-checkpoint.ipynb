{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from itertools import product\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open(\"/deep_data/data/land_cover_data/landcover_reproject.tif\")\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "tiles = [\"028012\", \"028011\"]\n",
    "\n",
    "landsat_dataset = list(i for i in range(len(tiles)))\n",
    "count = 0\n",
    "for tile in tiles:\n",
    "    landsat_dataset[count] = rasterio.open(\"/deep_data/data/combined/combined\" + tile + \".tif\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection:\n",
      "PROJCS[\"Albers\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378140,298.2569999999957,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]\n",
      "Labels projection:\n",
      "EPSG:32618\n"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_dataset[1].crs # 4326\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a randomly selected number of pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    total_count = train_count + val_count\n",
    "    for index, image_dataset in enumerate(image_datasets):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(product(rows, columns)), total_count)\n",
    "        \n",
    "        dataset_index = [index] * total_count\n",
    "        dataset_pixels = list(zip(points, dataset_index))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count]\n",
    "        val_pixels += dataset_pixels[train_count:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)\n",
    "\n",
    "\n",
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, band_count))\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile)\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "                middle_pixel_r = r + np.ceil(tile_width/2)\n",
    "                middle_pixel_c = c + np.ceil(tile_height/2)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(middle_pixel_r, middle_pixel_c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 11\n",
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "epochs = 100\n",
    "classNum = len(np.unique(label_image))\n",
    "(train_pixels, val_pixels) = make_pixel_locations(landsat_dataset, 25000, 10000, tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 7)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = tile_size, tile_size\n",
    "img_bands = landsat_dataset[0].count\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)\n",
    "\n",
    "print(len(train_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 128)         8192      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 100)         115300    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 23)                11799     \n",
      "=================================================================\n",
      "Total params: 187,003\n",
      "Trainable params: 187,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(100, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(classNum, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 64s 32ms/step - loss: 1.7545 - acc: 0.4497 - val_loss: 1.7533 - val_acc: 0.4603\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 1.6683 - acc: 0.4695 - val_loss: 1.6758 - val_acc: 0.4780\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 1.5977 - acc: 0.4896 - val_loss: 1.7581 - val_acc: 0.4698\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 1.5735 - acc: 0.4945 - val_loss: 1.6380 - val_acc: 0.4937\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 1.5846 - acc: 0.4854 - val_loss: 1.6481 - val_acc: 0.4882\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 1.5599 - acc: 0.4915 - val_loss: 1.7848 - val_acc: 0.3917\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 1.4891 - acc: 0.5191 - val_loss: 1.7805 - val_acc: 0.4280\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 1.5227 - acc: 0.5032 - val_loss: 1.7004 - val_acc: 0.4770\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 58s 29ms/step - loss: 1.5059 - acc: 0.5062 - val_loss: 1.6864 - val_acc: 0.4870\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 1.4567 - acc: 0.5253 - val_loss: 1.7644 - val_acc: 0.4458\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 1.4351 - acc: 0.5305 - val_loss: 1.6370 - val_acc: 0.5038\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 1.4333 - acc: 0.5284 - val_loss: 1.7316 - val_acc: 0.4780\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 1.4015 - acc: 0.5394 - val_loss: 1.7329 - val_acc: 0.4798\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 1.3433 - acc: 0.5597 - val_loss: 1.8665 - val_acc: 0.4585\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 1.3394 - acc: 0.5608 - val_loss: 1.9117 - val_acc: 0.4581\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 1.3076 - acc: 0.5697 - val_loss: 1.9418 - val_acc: 0.4783\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 1.2630 - acc: 0.5827 - val_loss: 2.3789 - val_acc: 0.3144\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 53s 26ms/step - loss: 1.2298 - acc: 0.5926 - val_loss: 1.8361 - val_acc: 0.4931\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 1.1985 - acc: 0.5993 - val_loss: 2.1140 - val_acc: 0.4552\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 1.1712 - acc: 0.6105 - val_loss: 2.1312 - val_acc: 0.4619\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 53s 26ms/step - loss: 1.1379 - acc: 0.6203 - val_loss: 2.4849 - val_acc: 0.3480\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 55s 27ms/step - loss: 1.1122 - acc: 0.6263 - val_loss: 2.3203 - val_acc: 0.4598\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 1.1011 - acc: 0.6290 - val_loss: 2.2812 - val_acc: 0.4338\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 1.0747 - acc: 0.6354 - val_loss: 2.7372 - val_acc: 0.3382\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 53s 27ms/step - loss: 1.0706 - acc: 0.6376 - val_loss: 2.3060 - val_acc: 0.4607\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 1.0273 - acc: 0.6506 - val_loss: 2.5269 - val_acc: 0.4489\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 1.0273 - acc: 0.6495 - val_loss: 2.5452 - val_acc: 0.4386\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 1.0010 - acc: 0.6586 - val_loss: 2.7284 - val_acc: 0.3844\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 55s 27ms/step - loss: 0.9871 - acc: 0.6612 - val_loss: 2.6594 - val_acc: 0.4271\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 0.9529 - acc: 0.6736 - val_loss: 2.7664 - val_acc: 0.4160\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.9423 - acc: 0.6779 - val_loss: 3.1302 - val_acc: 0.3893\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 53s 27ms/step - loss: 0.9623 - acc: 0.6708 - val_loss: 2.8575 - val_acc: 0.4386\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.9310 - acc: 0.6823 - val_loss: 2.6954 - val_acc: 0.4566\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 0.9118 - acc: 0.6885 - val_loss: 2.9240 - val_acc: 0.4186\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.8929 - acc: 0.6946 - val_loss: 3.4226 - val_acc: 0.3602\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 55s 27ms/step - loss: 0.8942 - acc: 0.6967 - val_loss: 2.8539 - val_acc: 0.4385\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.8718 - acc: 0.7020 - val_loss: 2.9895 - val_acc: 0.4214\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.8670 - acc: 0.7052 - val_loss: 3.5750 - val_acc: 0.3764\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 53s 27ms/step - loss: 0.8651 - acc: 0.7055 - val_loss: 3.2554 - val_acc: 0.4241\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.8401 - acc: 0.7129 - val_loss: 2.9898 - val_acc: 0.4344\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 0.8465 - acc: 0.7123 - val_loss: 3.3256 - val_acc: 0.4156\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.8345 - acc: 0.7165 - val_loss: 3.8306 - val_acc: 0.3769\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.8257 - acc: 0.7202 - val_loss: 3.2276 - val_acc: 0.4247\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.8010 - acc: 0.7276 - val_loss: 3.0397 - val_acc: 0.4387\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.8203 - acc: 0.7212 - val_loss: 3.9981 - val_acc: 0.3093\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 53s 27ms/step - loss: 0.8127 - acc: 0.7254 - val_loss: 3.6156 - val_acc: 0.4207\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7870 - acc: 0.7323 - val_loss: 3.2624 - val_acc: 0.4350\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 0.7665 - acc: 0.7405 - val_loss: 3.5507 - val_acc: 0.4319\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.7724 - acc: 0.7406 - val_loss: 3.8159 - val_acc: 0.3651\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.7693 - acc: 0.7404 - val_loss: 3.6061 - val_acc: 0.4239\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.7515 - acc: 0.7462 - val_loss: 3.2156 - val_acc: 0.4378\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7370 - acc: 0.7499 - val_loss: 4.4262 - val_acc: 0.3254\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 53s 27ms/step - loss: 0.7536 - acc: 0.7482 - val_loss: 3.9411 - val_acc: 0.3967\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7088 - acc: 0.7600 - val_loss: 3.5943 - val_acc: 0.4195\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7169 - acc: 0.7593 - val_loss: 3.4628 - val_acc: 0.4379\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.7565 - acc: 0.7483 - val_loss: 4.3150 - val_acc: 0.3397\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.7337 - acc: 0.7544 - val_loss: 3.7832 - val_acc: 0.4211\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7046 - acc: 0.7644 - val_loss: 3.3621 - val_acc: 0.4464\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7147 - acc: 0.7635 - val_loss: 4.4360 - val_acc: 0.3483\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 52s 26ms/step - loss: 0.7377 - acc: 0.7549 - val_loss: 4.0109 - val_acc: 0.4176\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7115 - acc: 0.7652 - val_loss: 4.0596 - val_acc: 0.4167\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 55s 27ms/step - loss: 0.7045 - acc: 0.7682 - val_loss: 3.6089 - val_acc: 0.4298\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.7360 - acc: 0.7558 - val_loss: 4.6601 - val_acc: 0.3388\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.7201 - acc: 0.7643 - val_loss: 3.9577 - val_acc: 0.4219\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.7064 - acc: 0.7652 - val_loss: 3.6711 - val_acc: 0.4316\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7137 - acc: 0.7658 - val_loss: 4.2052 - val_acc: 0.3938\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.7349 - acc: 0.7583 - val_loss: 4.1404 - val_acc: 0.4189\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 58s 29ms/step - loss: 0.6881 - acc: 0.7745 - val_loss: 4.2980 - val_acc: 0.4197\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 0.6666 - acc: 0.7790 - val_loss: 3.7605 - val_acc: 0.4292\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.6879 - acc: 0.7742 - val_loss: 5.0155 - val_acc: 0.3390\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.7005 - acc: 0.7719 - val_loss: 4.4092 - val_acc: 0.4195\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 0.7097 - acc: 0.7699 - val_loss: 4.0188 - val_acc: 0.4169\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 0.6614 - acc: 0.7849 - val_loss: 4.5259 - val_acc: 0.3501\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.6876 - acc: 0.7757 - val_loss: 4.4784 - val_acc: 0.4122\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 58s 29ms/step - loss: 0.6650 - acc: 0.7836 - val_loss: 4.6139 - val_acc: 0.4157\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 0.6824 - acc: 0.7836 - val_loss: 4.1598 - val_acc: 0.4353\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.6559 - acc: 0.7882 - val_loss: 5.1592 - val_acc: 0.3909\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.6289 - acc: 0.7937 - val_loss: 4.5149 - val_acc: 0.4133\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 0.6502 - acc: 0.7922 - val_loss: 4.5132 - val_acc: 0.4260\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.6828 - acc: 0.7825 - val_loss: 4.6690 - val_acc: 0.3770\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.7035 - acc: 0.7750 - val_loss: 4.5943 - val_acc: 0.3982\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 0.6635 - acc: 0.7896 - val_loss: 4.8434 - val_acc: 0.4155\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 0.6856 - acc: 0.7845 - val_loss: 4.3955 - val_acc: 0.4279\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 55s 27ms/step - loss: 0.6569 - acc: 0.7909 - val_loss: 5.3521 - val_acc: 0.3781\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.6285 - acc: 0.7955 - val_loss: 4.7324 - val_acc: 0.4197\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 58s 29ms/step - loss: 0.6735 - acc: 0.7901 - val_loss: 4.5409 - val_acc: 0.4159\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.6858 - acc: 0.7847 - val_loss: 4.5929 - val_acc: 0.4047\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 0.6766 - acc: 0.7846 - val_loss: 4.8108 - val_acc: 0.3961\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.6709 - acc: 0.7896 - val_loss: 4.3883 - val_acc: 0.4174\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 0.7158 - acc: 0.7823 - val_loss: 4.4416 - val_acc: 0.4214\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.6721 - acc: 0.7896 - val_loss: 4.8535 - val_acc: 0.4101\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.6487 - acc: 0.7927 - val_loss: 4.5741 - val_acc: 0.4159\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 57s 29ms/step - loss: 0.6557 - acc: 0.7921 - val_loss: 4.5615 - val_acc: 0.4153\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.6416 - acc: 0.7980 - val_loss: 4.7624 - val_acc: 0.4129\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 0.6515 - acc: 0.7938 - val_loss: 5.1454 - val_acc: 0.3968\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.6389 - acc: 0.8011 - val_loss: 4.5893 - val_acc: 0.4185\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 57s 28ms/step - loss: 0.6052 - acc: 0.8086 - val_loss: 4.8177 - val_acc: 0.4080\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.6355 - acc: 0.8022 - val_loss: 5.1484 - val_acc: 0.3984\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 53s 27ms/step - loss: 0.6739 - acc: 0.7913 - val_loss: 4.7014 - val_acc: 0.4213\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.6879 - acc: 0.7900 - val_loss: 4.6014 - val_acc: 0.4115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f496fe46e10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator(landsat_dataset, label_dataset, tile_size, tile_size, train_pixels, batch_size), \n",
    "                    steps_per_epoch=len(train_pixels) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_dataset, label_dataset, tile_size, tile_size, val_pixels, batch_size),\n",
    "                    validation_steps=len(val_pixels) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
