{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN Framework for Landsat Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from itertools import product\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open('/deep_data/landcover_reproject.tif')\n",
    "label_image = label_dataset.read()\n",
    "landsat_dataset = rasterio.open('/deep_data/LC08_CU_028012_20140814_20171017_C01_V01_SR/combined.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection:\n",
      "PROJCS[\"Albers\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378140,298.2569999999957,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]\n",
      "Labels projection:\n",
      "EPSG:32618\n"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_dataset.crs # 4326\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator and Prep Fcns\n",
    "\n",
    "This is a typical Keras generator that I've written to allow it to ingest a set of random pixel locations so we can randomly sample throughout the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, band_count))\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile)\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "                middle_pixel_r = r + np.ceil(tile_width/2)\n",
    "                middle_pixel_c = c + np.ceil(tile_height/2)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(middle_pixel_r, middle_pixel_c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of raster datasets and randomly samples `train_count` and `val_count` random pixels from each dataset.\n",
    "\n",
    "It doesn't sample within tile_size / 2 of the edge in order to avoid missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a randomly selected number of pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    total_count = train_count + val_count\n",
    "    for index, image_dataset in enumerate(image_datasets):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(product(rows, columns)), total_count)\n",
    "        \n",
    "        dataset_index = [index] * total_count\n",
    "        \n",
    "        dataset_pixels = list(zip(points, dataset_index))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count]\n",
    "        val_pixels += dataset_pixels[train_count:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the generator and data prep functions\n",
    "\n",
    "Let's make sure all this data prep actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and validation pixel locations\n",
    "train_px, val_px = make_pixel_locations([landsat_dataset], 100, 20, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# print out some image and label batches and check out their shapes\n",
    "im_batch = None\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_generator([landsat_dataset], label_dataset, 11, 11, train_px, 2):\n",
    "    if count > 3:\n",
    "        break\n",
    "    print('Image')\n",
    "    print(im.shape)\n",
    "    print('Label')\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect an image patch\n",
    "\n",
    "While it shouldn't necessarily be recognizable it should look like it has data in it and that it varies somewhat from pixel to pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffb89f78780>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(18, 5)) \n",
    "\n",
    "axs.imshow(im_batch[0,:,:,1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to the CNN Development!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 7)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "#num_classes = len(np.unique(label_image))\n",
    "epochs = 100\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 11, 11\n",
    "img_bands = landsat_dataset.count\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the model\n",
    "\n",
    "This is just a simple CNN model but it should be able to perform well above random when predicting landcover types if everything is correct thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 9, 9, 11)          704       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 22)          2200      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 44)          8756      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 128)         50816     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 44)                5676      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 23)                1035      \n",
      "=================================================================\n",
      "Total params: 69,187\n",
      "Trainable params: 69,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(11, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(22, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(44, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(44, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the train/validation pixel locations to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_px, val_px = make_pixel_locations(image_datasets=[landsat_dataset], \n",
    "                                       train_count=10000, val_count=300, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up the remaining model hyperparameters and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 1.9665 - acc: 0.3523 - val_loss: 1.7410 - val_acc: 0.3867\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.8697 - acc: 0.3704 - val_loss: 1.7511 - val_acc: 0.3567\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.8244 - acc: 0.3865 - val_loss: 1.7662 - val_acc: 0.3667\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.7902 - acc: 0.3948 - val_loss: 1.7243 - val_acc: 0.3600\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7616 - acc: 0.4008 - val_loss: 1.7139 - val_acc: 0.3433\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.7297 - acc: 0.4115 - val_loss: 1.7306 - val_acc: 0.3100\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7097 - acc: 0.4188 - val_loss: 1.7447 - val_acc: 0.3567\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6902 - acc: 0.4232 - val_loss: 1.8117 - val_acc: 0.3200\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.6790 - acc: 0.4298 - val_loss: 1.7863 - val_acc: 0.3133\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.6537 - acc: 0.4355 - val_loss: 1.8007 - val_acc: 0.3167\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6399 - acc: 0.4418 - val_loss: 1.8899 - val_acc: 0.2733\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6118 - acc: 0.4511 - val_loss: 1.9231 - val_acc: 0.3000\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.5991 - acc: 0.4549 - val_loss: 1.7722 - val_acc: 0.3367\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5909 - acc: 0.4623 - val_loss: 1.8029 - val_acc: 0.3567\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5634 - acc: 0.4694 - val_loss: 1.8831 - val_acc: 0.3267\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5190 - acc: 0.4827 - val_loss: 1.8415 - val_acc: 0.3233\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5063 - acc: 0.4865 - val_loss: 1.9149 - val_acc: 0.3100\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.5101 - acc: 0.4879 - val_loss: 1.9817 - val_acc: 0.2567\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.5083 - acc: 0.4884 - val_loss: 1.8856 - val_acc: 0.3100\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4728 - acc: 0.5015 - val_loss: 2.0896 - val_acc: 0.3100\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.4477 - acc: 0.5065 - val_loss: 1.9428 - val_acc: 0.3100\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4234 - acc: 0.5167 - val_loss: 2.0227 - val_acc: 0.3200\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4311 - acc: 0.5169 - val_loss: 2.2195 - val_acc: 0.3233\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4229 - acc: 0.5204 - val_loss: 1.9549 - val_acc: 0.3367\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4208 - acc: 0.5229 - val_loss: 2.0446 - val_acc: 0.3033\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4426 - acc: 0.5237 - val_loss: 1.8570 - val_acc: 0.3300\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3804 - acc: 0.5413 - val_loss: 2.3077 - val_acc: 0.3067\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3953 - acc: 0.5373 - val_loss: 2.2564 - val_acc: 0.3000\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3813 - acc: 0.5450 - val_loss: 1.9383 - val_acc: 0.3400\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.4324 - acc: 0.5286 - val_loss: 2.2449 - val_acc: 0.3300\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3562 - acc: 0.5471 - val_loss: 2.0643 - val_acc: 0.2933\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3485 - acc: 0.5578 - val_loss: 2.0779 - val_acc: 0.3033\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3120 - acc: 0.5646 - val_loss: 2.2222 - val_acc: 0.3100\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3089 - acc: 0.5685 - val_loss: 2.1239 - val_acc: 0.2967\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3360 - acc: 0.5601 - val_loss: 2.1944 - val_acc: 0.3233\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2759 - acc: 0.5760 - val_loss: 2.2455 - val_acc: 0.3167\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3015 - acc: 0.5668 - val_loss: 2.3266 - val_acc: 0.2733\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3682 - acc: 0.5531 - val_loss: 2.1788 - val_acc: 0.2800\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3247 - acc: 0.5640 - val_loss: 2.3444 - val_acc: 0.3000\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2700 - acc: 0.5833 - val_loss: 2.2276 - val_acc: 0.2967\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2606 - acc: 0.5825 - val_loss: 2.2253 - val_acc: 0.3000\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2267 - acc: 0.5924 - val_loss: 2.3397 - val_acc: 0.2933\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2496 - acc: 0.5929 - val_loss: 2.4980 - val_acc: 0.2867\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2245 - acc: 0.5949 - val_loss: 2.3518 - val_acc: 0.3433\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2448 - acc: 0.5941 - val_loss: 2.0161 - val_acc: 0.3500\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2196 - acc: 0.5985 - val_loss: 2.2962 - val_acc: 0.3033\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3293 - acc: 0.5702 - val_loss: 2.4987 - val_acc: 0.3133\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2246 - acc: 0.6009 - val_loss: 2.1797 - val_acc: 0.3467\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2139 - acc: 0.6064 - val_loss: 2.1835 - val_acc: 0.3200\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2238 - acc: 0.6031 - val_loss: 2.1989 - val_acc: 0.2567\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2549 - acc: 0.5931 - val_loss: 2.0613 - val_acc: 0.3500\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1918 - acc: 0.6099 - val_loss: 1.9630 - val_acc: 0.3300\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2002 - acc: 0.6100 - val_loss: 2.0396 - val_acc: 0.3133\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2016 - acc: 0.6097 - val_loss: 2.2566 - val_acc: 0.3333\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1866 - acc: 0.6126 - val_loss: 2.1918 - val_acc: 0.3067\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2163 - acc: 0.6153 - val_loss: 2.3162 - val_acc: 0.2933\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2797 - acc: 0.5933 - val_loss: 2.3134 - val_acc: 0.3300\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2129 - acc: 0.6066 - val_loss: 2.3031 - val_acc: 0.2733\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2221 - acc: 0.6113 - val_loss: 2.3462 - val_acc: 0.3033\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1852 - acc: 0.6150 - val_loss: 2.2605 - val_acc: 0.3300\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2131 - acc: 0.6088 - val_loss: 2.6171 - val_acc: 0.3067\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 2.0336 - acc: 0.4296 - val_loss: 1.9924 - val_acc: 0.3233\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.8741 - acc: 0.4209 - val_loss: 2.1263 - val_acc: 0.2933\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6627 - acc: 0.4776 - val_loss: 2.2062 - val_acc: 0.3467\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.4355 - acc: 0.5448 - val_loss: 2.1835 - val_acc: 0.3600\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3395 - acc: 0.5728 - val_loss: 2.2894 - val_acc: 0.2833\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2894 - acc: 0.5879 - val_loss: 2.4162 - val_acc: 0.3100\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2696 - acc: 0.5975 - val_loss: 2.6825 - val_acc: 0.2467\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2860 - acc: 0.5913 - val_loss: 2.4323 - val_acc: 0.2267\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2542 - acc: 0.5967 - val_loss: 2.4239 - val_acc: 0.2933\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1960 - acc: 0.6162 - val_loss: 2.1764 - val_acc: 0.3733\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2227 - acc: 0.6107 - val_loss: 2.5960 - val_acc: 0.2933\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2184 - acc: 0.6105 - val_loss: 2.5864 - val_acc: 0.2900\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3317 - acc: 0.5868 - val_loss: 2.2913 - val_acc: 0.3367\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2217 - acc: 0.6097 - val_loss: 2.9515 - val_acc: 0.2533\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1880 - acc: 0.6170 - val_loss: 2.7568 - val_acc: 0.3200\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2313 - acc: 0.6057 - val_loss: 2.2282 - val_acc: 0.3500\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1570 - acc: 0.6267 - val_loss: 2.7305 - val_acc: 0.2733\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1725 - acc: 0.6293 - val_loss: 2.8362 - val_acc: 0.2767\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1547 - acc: 0.6290 - val_loss: 2.7469 - val_acc: 0.2867\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1847 - acc: 0.6259 - val_loss: 2.5515 - val_acc: 0.2600\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1470 - acc: 0.6335 - val_loss: 2.4428 - val_acc: 0.3000\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2097 - acc: 0.6142 - val_loss: 2.5620 - val_acc: 0.3333\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2240 - acc: 0.6235 - val_loss: 2.5495 - val_acc: 0.2800\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1596 - acc: 0.6306 - val_loss: 2.6882 - val_acc: 0.2533\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1975 - acc: 0.6250 - val_loss: 2.8829 - val_acc: 0.2433\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1362 - acc: 0.6374 - val_loss: 2.4614 - val_acc: 0.2833\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1352 - acc: 0.6392 - val_loss: 2.4352 - val_acc: 0.2700\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1436 - acc: 0.6374 - val_loss: 2.5146 - val_acc: 0.2833\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1227 - acc: 0.6412 - val_loss: 2.6880 - val_acc: 0.2667\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1642 - acc: 0.6338 - val_loss: 2.7989 - val_acc: 0.2600\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1330 - acc: 0.6474 - val_loss: 2.7516 - val_acc: 0.2767\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1623 - acc: 0.6315 - val_loss: 2.5551 - val_acc: 0.3100\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1063 - acc: 0.6489 - val_loss: 2.7606 - val_acc: 0.2700\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.0875 - acc: 0.6541 - val_loss: 2.5889 - val_acc: 0.2200\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1425 - acc: 0.6450 - val_loss: 2.7921 - val_acc: 0.2900\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.0959 - acc: 0.6536 - val_loss: 2.8151 - val_acc: 0.2700\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.0528 - acc: 0.6649 - val_loss: 2.4596 - val_acc: 0.3000\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.0975 - acc: 0.6549 - val_loss: 2.6209 - val_acc: 0.2567\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.0740 - acc: 0.6601 - val_loss: 2.7128 - val_acc: 0.3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffafa1dacc0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator([landsat_dataset], label_dataset, 11, 11, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's evaluate the Model\n",
    "\n",
    "We'll just generate 500 test pixels to evaluate it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'landsat_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1caaadcb9f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_px\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_px\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pixel_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlandsat_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtile_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'landsat_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_px, val_px = make_pixel_locations([landsat_dataset], train_count=500, val_count=0, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a built in evaluate_generator function and because we told it above to use accuracy as a metric this function automatically outputs categorical accuracy which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 8s 376ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9832591772079469, 0.3760000020265579]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this simple model we're getting 37% accuracy across 23 classes which is well above the random accuracy which would be around 4% (aka 1/23). That means we're in business!\n",
    "\n",
    "### Evaluating the model ourselves\n",
    "\n",
    "If we wanted to run this evaluation and take a look at specific predictions and labels we can do that below (albeit more inefficiently) just to get an intuitive understanding of what is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 7s 373ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(test_px):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 23)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 14,  5, 13, 13, 20,  8, 21, 13, 12])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = np.zeros(pred_index.shape)\n",
    "correct_predictions[label_index == pred_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1.])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.376"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(correct_predictions) / len(test_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.5005555e-05, 2.5345067e-05, 6.9519803e-03, 9.5544951e-03,\n",
       "        1.7500592e-02, 4.5062839e-03, 7.9424324e-05, 5.6560391e-05,\n",
       "        4.5279139e-03, 8.9906462e-05, 8.1206148e-04, 4.5664867e-05,\n",
       "        5.2689546e-04, 2.0000549e-02, 8.8348606e-04, 2.6076639e-02,\n",
       "        3.8386403e-05, 7.5204247e-05, 5.2162164e-01, 3.7951332e-02,\n",
       "        6.4366475e-02, 2.8416619e-01, 6.7970672e-05],\n",
       "       [9.6968334e-04, 6.7671924e-04, 1.0775626e-02, 1.2592370e-02,\n",
       "        1.2026070e-02, 1.2465723e-02, 3.3110134e-02, 1.2139344e-03,\n",
       "        2.5713392e-02, 2.2136976e-03, 3.7321962e-02, 2.8844578e-03,\n",
       "        6.2341351e-02, 1.9656718e-01, 1.5323413e-02, 1.5057945e-01,\n",
       "        4.1870007e-04, 6.7505526e-04, 8.9561611e-02, 1.7911386e-02,\n",
       "        2.0733811e-02, 2.9347154e-01, 4.5277303e-04],\n",
       "       [2.0889305e-04, 1.2599016e-04, 9.1587409e-04, 8.8419262e-03,\n",
       "        5.2371044e-02, 3.6170930e-02, 3.1249449e-01, 5.3113926e-04,\n",
       "        1.3841026e-01, 6.5567750e-03, 1.9548453e-01, 3.2652885e-02,\n",
       "        9.5884621e-02, 5.9320562e-02, 4.4410080e-02, 5.9092906e-03,\n",
       "        1.7554613e-04, 1.5845097e-04, 1.2757443e-03, 1.7050681e-04,\n",
       "        6.1450154e-03, 1.6437772e-03, 1.4168759e-04]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10,  6, 13, 13, 21, 13, 13,  6,\n",
       "       13,  6, 21])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0,  6,  0,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 43,  0,  0,  4,  0,  0,  8,  0,  0,  0,  0,  3],\n",
       "       [ 0,  3,  0,  9,  0,  0,  6,  0,  0,  3,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0, 25,  2,  0,  0,  0,  0, 32,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0, 11,  0,  0,  0,  0,  2],\n",
       "       [ 0,  6,  0,  8,  0,  0,  9,  0,  0, 25,  0,  0,  3,  0,  0],\n",
       "       [ 0,  6,  0, 26,  0,  0, 14,  0,  0, 86,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  5,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  3,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  0,  0,  3,  0,  0,  2,  0, 47]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(label_index, pred_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
