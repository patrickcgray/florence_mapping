{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN Framework for Landsat Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open('/deep_data/landcover_reproject.tif')\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "image_paths = ['/deep_data/processed_landsat/LC08_CU_027012_20170907_20181121_C01_V01_SR_combined.tif',\n",
    "               '/deep_data/processed_landsat/LC08_CU_028012_20140814_20171017_C01_V01_SR_combined.tif',\n",
    "               '/deep_data/processed_landsat/LC08_CU_028011_20170907_20181130_C01_V01_SR_combined.tif',  \n",
    "               '/deep_data/processed_landsat/LC08_CU_028012_20171002_20171019_C01_V01_SR_combined.tif']\n",
    "\n",
    "landsat_datasets = []\n",
    "for fp in image_paths:\n",
    "    landsat_datasets.append(rasterio.open(fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection:\n",
      "PROJCS[\"Albers\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378140,298.2569999999957,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]\n",
      "---\n",
      "Labels projection:\n",
      "EPSG:32618\n"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_datasets[0].crs # 4326\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "print('---')\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Dictionary and Confusion Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dict((\n",
    "(0,  'Background'),\n",
    "(1, 'Unclassified'),\n",
    "(2, 'High Intensity Developed'),\n",
    "(3, 'Medium Intensity Developed'),\n",
    "(4, 'Low Intensity Developed'),\n",
    "(5, 'Open Space Developed'),\n",
    "(6, 'Cultivated Land'),\n",
    "(7, 'Pasture/Hay'),\n",
    "(8, 'Grassland'),\n",
    "(9, 'Deciduous Forest'),\n",
    "(10, 'Evergreen Forest'),\n",
    "(11, 'Mixed Forest'),\n",
    "(12, 'Scrub/Shrub'),\n",
    "(13, 'Palustrine Forested Wetland'),\n",
    "(14, 'Palustrine Scrub/Shrub Wetland'),\n",
    "(15, 'Palustrine Emergent Wetland'),\n",
    "(16, 'Estuarine Forested Wetland'),\n",
    "(17, 'Estuarine Scrub/Shrub Wetland'),\n",
    "(18, 'Estuarine Emergent Wetland'),\n",
    "(19, 'Unconsolidated Shore'),\n",
    "(20, 'Bare Land'),\n",
    "(21, 'Water'),\n",
    "(22, 'Palustrine Aquatic Bed'),\n",
    "(23, 'Estuarine Aquatic Bed'),\n",
    "(24, 'Tundra'),\n",
    "(25, 'Snow/Ice')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTO Run: \\n\\nnp.set_printoptions(precision=2)\\n\\n# Plot non-normalized confusion matrix\\nplot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\\n                      class_dict=class_names)\\n\\n# Plot normalized confusion matrix\\nplot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\\n                      class_dict=class_names,\\n                      normalize=True)\\n\\nplt.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, class_dict,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    # convert class_id to class_name using the class_dict\n",
    "    cover_names = []\n",
    "    for cover_class in classes:\n",
    "        cover_names.append(class_dict[cover_class])\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        pass\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=cover_names, yticklabels=cover_names,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\"\"\"\n",
    "TO Run: \n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(np.argmax(sk_label_batch_val, axis=1), pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator and Prep Fcns\n",
    "\n",
    "This is a typical Keras generator that I've written to allow it to ingest a set of random pixel locations so we can randomly sample throughout the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, 3)) # take one off because we don't want the QA band\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            elif tile.shape != (band_count, tile_width, tile_height):\n",
    "                print('wrong shape')\n",
    "                print(tile.shape)\n",
    "                # somehow we're randomly getting tiles without the correct dimensions\n",
    "                pass\n",
    "            elif np.isin(tile[7,:,:], [352, 368, 392, 416, 432, 480, 840, 864, 880, 904, 928, 944, 1352]).any() == True:\n",
    "                # make sure pixel doesn't contain clouds\n",
    "                # this is probably pretty inefficient but only checking width x height for each tile\n",
    "                # read more here: https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1873_US_Landsat_ARD_DFCB_0.pdf\n",
    "                #print('Found some cloud.')\n",
    "                #print(tile[7,:,:])\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile[0:7])\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "                #print(reshaped_tile.shape)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile[:,:,1:4]\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of raster datasets and randomly samples `train_count` and `val_count` random pixels from each dataset.\n",
    "\n",
    "It doesn't sample within tile_size / 2 of the edge in order to avoid missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a train_count + val_count number of random pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    train_count_per_dataset = math.ceil(train_count / len(image_datasets))\n",
    "    val_count_per_dataset = math.ceil(val_count / len(image_datasets))\n",
    "   \n",
    "    total_count_per_dataset = train_count_per_dataset + val_count_per_dataset\n",
    "    for index, image_dataset in enumerate(image_datasets):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(itertools.product(rows, columns)), total_count_per_dataset)\n",
    "        \n",
    "        dataset_index_list = [index] * total_count_per_dataset\n",
    "        \n",
    "        dataset_pixels = list(zip(points, dataset_index_list))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count_per_dataset]\n",
    "        val_pixels += dataset_pixels[train_count_per_dataset:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the generator and data prep functions\n",
    "\n",
    "Let's make sure all this data prep actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and validation pixel locations\n",
    "train_px, val_px = gen_pixel_locations(landsat_datasets, 100, 20, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 64, 64, 3)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# print out some image and label batches and check out their shapes\n",
    "im_batch = None\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_generator(landsat_datasets, label_dataset, 64, 64, train_px, 2):\n",
    "    if count > 3:\n",
    "        break\n",
    "    print('Image')\n",
    "    print(im.shape)\n",
    "    print('Label')\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect an image patch\n",
    "\n",
    "While it shouldn't necessarily be recognizable it should look like it has data in it and that it varies somewhat from pixel to pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff9af97e240>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnW2sZedV3/9rv5yXO3NnxuM33DglrkpBfCgJslIiECpJg9IUkXyIIhCq3MqSv9AqCCTiUKkSVT+EL0CkVkEWAVyJkqQB6iiiQBqCKqTKxCEBkpgQkzqNXb97Xu+555z98vTDOfbc9V9rzt5z586ZO1vrJ1mevc8+z372y3nu3v/nv9aSlBKCIAiGQnazOxAEQXCUxKAWBMGgiEEtCIJBEYNaEASDIga1IAgGRQxqQRAMihjUgiAYFDGoBUEwKK5rUBORd4nI10XkKRF5+Kg6FQRBcFjksBEFIpID+FsA7wTwDIAvAPjJlNLXrvadLM9TXpavL6e20W1mdozl/qWm2fg5RPp0fyNZXph1kul2U9tu7NeR4B1Lx/XKypFabqulbYJ3c6398r7U1ajTbbtKrxFu5PovrduPLoSug9dEVpRqWUTfy21Tb2wTAFLquKf63Nsdvwf+vRzFKe1D5z3X415vm/bllNKdXfuyv97+vBXAUymlb676JB8H8B4AVx3U8rLE2Tf+/deXq9kF9fnoxAnznWqxUMvzvUtquV7omyUrejx88m9FcrU8PXnGfKU8MaV+XVbLiwsX1bL7x4JWZc4gTh1z2qAbv9WNnrz7O9XypRf+r2mihW4jowFbuKPur5i+0/FjaltnCKN1iXZk+pXZ88HbmB9xQ/twrgtvw5Rj/Yeico5l9443qOWinKjlvYvn1PJ4rAdBAFgu9b29vLinls0ffbH3Dw+eWabv7aap9Oc97jE7InUPhWaA5tuD+iXOgwQ/9Mxeufitzh3j+l4/3wDg2weWn1mvU4jIQyLyhIg80d6Ip5kgCIID3PCJgpTSIyml+1NK92d53v2FIAiC6+B6Xj+fBfDGA8v3rtddldS2qBbz15fz6Y76PBvb7oxGtI5eL5eXZ2q5qa2GxM++/DTNj8re+5YkPSDnI9JQSFNp6bUZACTXfTf6Rof+4XWNt6gX+7RT51iusVHvtQ/0KmQkNNIcvRcWoT9y/PrJfc8Ke38c1GgBgN8GEr1uuS9O/KedulEt9T3VOo2cf1Xf+nfd84/UckGvVxVfJwDN/lwtZ3y/UO89vZSp6XU0z/X5Kkf61RoA6oXuR0L378PQ8cpaTKb0uX2+apxz1IfreVL7AoDvEpH7RGQE4CcAfPo62guCILhuDv2kllKqReTfAPgjADmA30gpffXIehYEQXAIruf1EymlPwDwB0fUlyAIguvmuga1ayWlhHp55T1ZCq2pzS/M+CtISWsCxY6eKp+e2VXL1cxqWct9/W7OOgsLLYuZnloHgKbWWs34lO57OdX9amrd79V+aEdGy2NrhcVIFcbSQIqCo8sdgZXPtMG+LHMsni5HsB2hc6cA8lLbgNpW2yCQUZsd9o3Vfni37FG0s/gtXW8+3tFY3x9z5z4dTU+p5Sbp+7Rd0nJF9zGsPsxnrKFz3M7tOWcZ1njqWKPuof3y+ShGY92Pln8LQHPIGzXCpIIgGBQxqAVBMChiUAuCYFDEoBYEwaDY6kQBkJRQ2Vw8Tx87YywJn22lhc3JbSf18ikbP8os9jcHDhsTqEMiB6aM9fLJ8jvMdy6fe15/h4PkzcSBcz5YtCYt1Yjt7mxDh4O3R8wlB5tz/GxW0q3liMlNw2L55n7xZA0A1HQ/dIXi+Vd2c/Q9f1qWYzANB7DT8e7u6nji+VzHDgNA25DJd6lNsDwZkeWOjbrd3I/Oa++v2oz3BepaVpB5XfQ5zHI76VEdckIrntSCIBgUMagFQTAoYlALgmBQbN18m9IVzYP1D85BBQA5B4qThlJd1rpMedK+iBcT3e5yTgG7Yo1/TLXUBt6pnFbLIrSPBZlA4eQp6wpg90yNbEikvFQmoNu20JkY0FPQDBzQLhysz7qlY/Kkv6kZ6XItJxH1DJpLMmwbXZL22euE8Aq9nDuB9Xws1b7WzDK6B2tHU0sVB3AbBystOleKc66R2ZgD3N1EA3SPmZx1RjCzbbAuzYH0KeO8bvacFo522Yd4UguCYFDEoBYEwaCIQS0IgkGxVU1Nsgz5+Mp7crOndae8tJlxyx3tO2vmWkOrKVjdq1GQUTCxDb6m7XObP94mDtS6nJAuVzlB8Zwv3tjDOGmiIw+1iXTIkQ6st8HHtg0TbG4kk80BzID19rWb49mVlnplJefc39yGS+KCJhzQT9sXnj64+ZyxJ4+1PgBoGn0/LOf6+lcLfZ2mE+2vBIBFYo/hZs3VJm8E2oa0zK6T6J0OSk7JujZj/ZW2YdOGycxg29ghb98FvLCxH68RT2pBEAyKGNSCIBgUMagFQTAothv7mRLaA74ZEz/ojrFav8hIE6koHq6aO8n3qG5jbopN6Bf8ckRFIQAjcAnpGXOKY/WK1bJWx3GbDRf4cHSorNDenfGOTixY7VE8rSupdOgs3DFvc/KMcYwh+5Ikc241o22SHlSR8OLU2zRd44IvvEFXrVVYjxXXpMy9Y+EiOh3JGqcnToGpKyq8Qhosa39Nbe/1iuJp2R/ImmNe2sIrBfnwhMx9Ta09Zqmxehj/xtpW39vNrLtAkFeMpQ/xpBYEwaCIQS0IgkERg1oQBIMiBrUgCAbFdgPaAbTtFeFyTMZar1oQa+UZVeTOaRKgdiYKypPafDuaUII6MkJ6ftWUk7n2sjZXmio+nsjZIVI3vGcWigFkFOSbkdDLpuB+DtbNJEegt4kUKTidjcSOgTfL9XVpqXoSB7BzZTGnG0ZMNwk/3cpHdCx03suxnjhqnWSVbILmSaGaK8U790JVUXIGqriUJ90vb2Itz/X9kEY0GUdJI8ZTO2FR8D01e1UtN9RPY14G0NLxdlW58kucbU74eTXiSS0IgkERg1oQBIMiBrUgCAbFdgPaJcNockVHM9ZLJ5FgvdCmvVGhNYF8qnWHemkLONSUFPLE6TvUcqopYDl3khGSwZATTbKG5GkmXB27mGodZlxSosGFPRY2S/I5a1i7MS30gDUmT2U0RWMo2JrOV17YhH8tB0KzqVXIOGolRoCCy5Mx324uqgJYDU0y0m2p78kUjAEyTprJeyVzaeEkTSgnrDHr7/Bd6eYqoOtikigIJ+9kDRZYLi+q5XqhE3HytfaM5n6Q+8F+6GVXg+5o42rEk1oQBIMiBrUgCAZFDGpBEAyKrWpqmWQYl1c8PxUF5LIuBQBLSgIptE0+0svF1CnW0JD+Q0HwklFQb2b9MUJtZBT0yz6lZmG1ChlTERlSRcY7lPDR6UdBwdWcjNL4wRxRjRMJWHWGkxE6bRhvF+mSlEQyt3HTGJXa/1XRd6TUmkrN/qjVnvR36FP2ixUj68tqSSMbkRfQ+NYmNuFBU2vtFxnpYeT1a51klRklUlzuc6JR0pgcDdoUqyn0iWdLWcFFp2GD4o0n0TwKWe2rS8s1dkG3Is7hiCe1IAgGRQxqQRAMis5BTUR+Q0ReFJGvHFh3VkQ+KyLfWP//thvbzSAIgn700dR+C8B/AvBfDqx7GMDnUkofFpGH18sf7G4qKf8KJxJsWyeGjLwqy31drGUE7Vsrp7YgcjPXegcnkiynFKc3I30EQEX+N9b2uAhGVlh9sKCYU/atVbVuY/eULpgMAA359vbPv6KWcycOz9BRWMQUZnE0E0PLOhz5x5x4SdZmRqQpVRQLmuX2dmVfWsooeSUdSzHWxTwAIBet244p1rOBvk517WhZtMyJFRM9P6TG8VNSYeaKCmKz5yx3/GHs9+LvFFSEaLF3wbRxJF5HwtbpZl/j4eI8PTqf1FJK/wvAq7T6PQAeXf/7UQDvPbIeBUEQXAeH1dTuTik9t/738wDuPqL+BEEQXBfXbelIKSXhZ+0DiMhDAB4CfMtGEATBUXLYJ7UXROQeAFj//8WrbZhSeiSldH9K6f4Y1IIguNEc9knt0wAeAPDh9f8f6/vFg1V2eF6ATY6AnShIFZkL6Rlxcseu3Sftp7qshWEWceczHdALOAG4FMCcqNp4ObFu05wmD0oKnG7p88VMC8UAUNFEAQcOj0YT+tw+RJvq8iTQ51Shu63sxIk1+W6ebPCMsyJckV5/h5MXNJUV1/m6HEaAFq58xckJOEjekc6Lkie9dF/bhs5hbg281gLdVV3d+ZyOv6XzXrV6ksM9P/yb6/LFugHtm1dYj7h3rIebouhj6fgdAP8bwHeLyDMi8iBWg9k7ReQbAP7ZejkIguCm0/mkllL6yat89I4j7ksQBMF1ExEFQRAMii0XXkkq4FZIQ8pGNhi9oHVV4uSM+hDqpaP/tKwbsN7DwefdwbVscuR+FmOrqbWcONFUgieTp5OMsOG+kiZStfQdR5bIKWC7nGjtykxmZ/Z8tJyM0xinuQ3bj4Z0J5tskJIzOqZXk/Sw49JNJtacfXJXX4dzF17SbZIOxVXgAVstvZpfVsvCEf279sLknFiBMHqhc7CsTZnvVPXGz1frNnbD1b5tI3oxJ83R7ML8RoGi4ESaVuv2iCe1IAgGRQxqQRAMihjUgiAYFNvV1FJCdcCvkwntfml1hpO7OgHIbKwT5xVU4NUrZsweKTYBt0suLGGaMNHXOfvQSA8yRUUACB2v8SElrSFkuaN31DoAmWUoLgCSOzqllFxohPbRcPSx87dP2KdH/kHyurGfbNVZPn69uJzTdREnsJ79Thz0nenjb1uruc5mm4Pvq6X2NcIJzm9BvjTS/zIqKsOB+ACQU6Fhr5i1wvOpcfGainXI7kI0fC050SYX/zHFjQFMplqnFC5eQzrl/iUbWF+WTsLXHsSTWhAEgyIGtSAIBkUMakEQDIqtFzMuDySpW+yR52zsFKMgTWj3zFm1XO3p9/n9hX03Z+2BPUWm8LDjmWHtgTWTgt7/C7Geo4yLXIjeT0vFi+ulU2iEpJiSYj0z9gM5OlSiOMQ2bS7EnHoUM0bOwbG0WNi/n/mItJl9fT+wj61wCp5woREW7/JMn5+9i07uBdI/WcsRSt6ZF/Zns6T4WJPQ0miQToJHOkcZXVsutCJOP4xvkzQ1Uwzb8f6xDy+j65TRsNEmq1OWlGizmOiCNzX5+OZiPWjLhVdop5t4UguCYFDEoBYEwaCIQS0IgkERg1oQBINiyxMFooTscqoF6Ka2SQDZ+HhyqicKFq02aHIiRgBYLkgsZdOvLRdt2mCfY0PCcEtVejjxJADkZKasuQ1OLNja8zE+oStMScbnUIvtXhsm6JkEaE7W6GUsTnQsLSh5Zc1Vve35YAOqCZSmYP3aEY6NIE/XsmnJOOtUxuJKYKYiOV18zzhrXcCbk2Qul5xEASgnJ3UTZHptaL/5eMfpBwf407EUnADUMRKb49Pf4SpouQk8Bxqe0KLrL1n3PcbV1/oST2pBEAyKGNSCIBgUMagFQTAoth7QXh8wmJb0zjy/wPoHUC/1y/mCgpynU60rJCeA+/xSGy4LSs5YL3SbHIwNWJXN6A5kpizHNhlhSxXH52RAZH1osqM1FgDIyMTJVbwbrmouTjEbCshua66ETt/xNLWuFazlePU9wJpaR6ENL6EhmW+NhtSxT8BWMTe6FLViNSersUriBAe6X5VjrC4m+l7OOfFCy4lJHTMyaap8ysy1b51zyssNf0cfP1d0B4A86b4tSRsXatMrMsNG4b7Ek1oQBIMiBrUgCAZFDGpBEAyKLWtqLaoD+tVoR793j8ZWI2ANrWQ9jN7VOcEfAJSUsI6LTzCcaHH9Lb3Ysg9LaNm2UM2p0IjZD+suVmPcIW9SRUVS2EPVOMVquSA0yxktFxrx2uDjZ+3OVqs1sIZogqvZH+Yk3mS9h69DS/qYpzFygHrboSF5uhwosWbq8LYtF/baTpP2YGbUr1FO137ftpFlfF/S8VK/bLEbmJuXE562nNzB0Rj53ubEA4mMbFlh9TM55PAUT2pBEAyKGNSCIBgUMagFQTAotqqpISW0Bzwt7O0pxtZjxroSxzLu7+lCLOzDAWD0nJqTRBrtxikSyzkRSQ8S0ln2Lp43bTTkTSpGXGiFfEmO3jGnGEKrmZkqKqYNTujoxzIe3EmPormcwJC1LSfxZps45rThDWifFu47y25Gy8nssbYc+8rXn5a9OEWhdUZ3ozZa57qwljsinWlBv4VqX+vNAFAUmxN+Nub34XgyWXejYi4lxXrWzn1qbhnyQrLWWVISyVUbHfflVYgntSAIBkUMakEQDIoY1IIgGBQxqAVBMCi2OlGQFwXOnL3j9eUZCZ95bs23OVV2ZtMiJ6hrnKR3TcXJFymQmiTozK18TVWKWOQno2QGKyZzkr/lTAejc4JLrrYEWOGXBXljJDUt2ASOZh8UjO1Vk0qy2VxrxHbPwGsMupvNtp75lg3LZhPpDkY3J4kNzTxv4M1YcLvCEwPUpnNOawoML7mqO59SZ/Ilp8SSRcbV1KlyfGEn5zgJJBvJubq6Z0ZezPQEHl8YrkafTW0CiMXeS2ZdH+JJLQiCQRGDWhAEg6JzUBORN4rI50XkayLyVRH5wHr9WRH5rIh8Y/3/2258d4MgCDbTR1OrAfxcSukvRGQXwBdF5LMA/hWAz6WUPiwiDwN4GMAHN7aUdGXqjOKXK9hKz0YDIU1gtKPfxeulbWNJlc9NAkPWclgvgg3YZg2NRSVxilHwfhoqJMKFrovctsEB3MawC04AaYP3OWCbNbRedPoiOUrenlM2vQr9jWXdyb0utB9OLJhRtXX3ULlYC2lbxiTsNNFRd6XTJAwADV2r207r54QxJXyYl04CiH2deJSTE7Au2dT298KaGtgTzYZmN9EkGZaFdWu6b2utLwNAU9mA/T50PqmllJ5LKf3F+t+XADwJ4A0A3gPg0fVmjwJ476F6EARBcIRc0+yniLwJwFsAPA7g7pTSc+uPngdw91W+8xCAhwA7MxcEQXDU9J4oEJGTAH4XwM+klC4e/Cyt5ub9p/KUHkkp3Z9Sut+LmQuCIDhKej06iUiJ1YD22yml31uvfkFE7kkpPSci9wB48eotrODCK6OpLiyymF3mr9gys+Q5qxcUsFtZjYC1LPa+cXAxaz2r72S0rLWbEWk3tTfGk6dselIXJq5ID0mOHpbner81aYwNaYpusLoJFCcBKOthzHL0Lf0d0sOcuPp2SQVf2Jdnqt10azdcNBh0Dk/cdqdp4+QZve7iZX0rzylRaeYUiDEeOtqk4ALJjscso3usKMlzJnr5wjnr45pdvkB9JV8jdcwrmmIwtb67E01a7yc1Sfut9nW/VxvdoIB2WfX4YwCeTCn98oGPPg3ggfW/HwDw2KF6EARBcIT0eVL7QQD/EsBfi8iX1+t+AcCHAXxSRB4E8C0A778xXQyCIOhP56CWUvoz+KmsAOAdR9udIAiC62O7hVeQVOK3looInzpli/e+ek4nW2SPVbWY0+eeHqa1mpy0ioYKosArvEKTHDyTm5V6ud63+mBJmlGedJuc9G85d5IAlqShGR8S62GmCb/Yht6Cljv0M7h1hqlFZwPT18261NVa3vgpHevJU7ebbe58w31qefktfU8tKioQ7ByLcICkMarp7+SOLre/UPNvuDTTPrTxSHsy2ZMI2GI2Od2XJRX7Xs7sPca6dLeN0bv4m79k/HNc7Aew57AnESYVBMGgiEEtCIJBEYNaEASDIga1IAgGxdbjlg4K2VxtfX/PES1pIoCFcE5QZyqFw6lsRJMJHDRfOgnryp0Tuk0KWF9SYLAXO8Fa8nJOlbIocNozmy7pfBhJv0uxhyNqGzqis72+deyXK5av2qBFNht3H0rnfnmigM85ALzy3Lf1NiSe56yUO+ePe2HvS544sOeDTeAVBXRPaKJActuP0XiHtuH9kHG2dKpJNV3VxnoI+Mawu3lzb/LKm5DpQzypBUEwKGJQC4JgUMSgFgTBoNiu+TYBzQEtZnZJmw05yBXwK3tvxJNuWKvh+G0KJB6NbfK90URrFcul1rYaqvpeZp6qxsU4OoyzXibGjljzbmOtU129O+OjbYMMykZ36WECNsdXdRd8Ybq24PvnwsvPmm3Ov/SMWubEiXxdvHOck96Vky47OaE12f09KkwCm0egpnsqJy2vyO3PtxF9Tnemu2r50uWX6Rv22suYqrzPuRBNH8GMte/ub9iOhPk2CIIgBrUgCIZFDGpBEAyKrWpqWZZhckCb2rvwivq8+828+108ORqccdlknIxPB5qXpS14Agq45aIQqdF7aR1JjRM4cuEVg3uwPTxkHfTxsh0kc3xZBXkMOTljTbqUG9BuquTSx0b7O4RvibS+pu045/D62u3Ja6gw9+SUTgA6nZ5Sy7nz05vNXlXLFZ3TqtaaMxfhAYCadNp9SqxgEiC4happmbxubdWt/fIpymiTnP1z7n0cPrUgCIIY1IIgGBYxqAVBMCi261NrW1VchLWr2tEIrCeIi4bwPjzthhbJQzbZ0ckpc6fqVUWaCXuGEpmMvAIwzUKv44SX/aDYvQ472GFi6jL+W+f5sjrKHXIcoyuZ8HWh02GvpVfgg4oEm0bTpkW3c0YP2pzvcdUE3VOnztyhljmh43xfJz8FgJo0Vo7trBtOrGjPBxcvYgUxG5EHzUmI2pI+zFXgCoqvnnOxcADjifbpFXQsppC1c06bukdRGId4UguCYFDEoBYEwaCIQS0IgkERg1oQBINi60kiD46jo6k2cDaXL/LGyHI9mdCSqdMo0F4xcRJDxzQxUI61qJnYKbjasV5ecvUbmiiYOybPzqLmPSpB8Z+hDi3dM9ryRIGdbODq6vZ81CTi5lShPqdGU+uIvlz5izvC8xVuWaOO698r9poD1vnzzZMPqyb0RMDzT31Fb0CzDeJMRvF92lJwOk++5FQZCgBAk29jurfH482macBOSJRj2s+SJmcy28Z4RyeAMOb1Oc0KeZNm12gSf414UguCYFDEoBYEwaCIQS0IgkGx5YD2HCdOXklaJ2TgnF2+YL5jJBAyOTZs4nMS500owaMx/dZUzMQJaE8kZrHZkv86NJnVkIy+1XRoOQ4c5G0W2UjqmJGvNQS+KK12w5XAu8yUzcLTTKg4Cce3GynvEBrLtReb7wz4985pogPOMn1+hE2vvZ4ndOfnVDRmsqOD5gGgoOSUGd3LbOg9MTlj2jCJWblwEd0Oo1YnogSAgn5zaaGL2TSV/n2w8Rg4bDh7PKkFQTAwYlALgmBQxKAWBMGg2K5PLQNkfGWXHCg7ooLBAJDY30JJ/5ZLCgJOdpxuyGfE/p9S9GmonGB01ky4WAkHwY927bFUe1oTaZZaVzDB557nzqzrTJvZpxH9MRfJdXxZDSeFpASYqSaPVe0kKyCrlhTs3eoT0L4Zlse8wGlbR6TjOnCEO4BE+mixoxOPJvrOKHd0ypH+zt6cgtOpMNHU8brxsbT0ndFUa101FyWCTbyQhIp/k59OxA4jKelj4USjtlCPaeJqKzuJJ7UgCAZFDGpBEAyKzkFNRCYi8uci8pci8lUR+cX1+vtE5HEReUpEPiEio662giAIbjR9NLUFgLenlC6LSAngz0TkfwD4WQC/klL6uIj8GoAHAXx0U0OpaTC/fO7Kcq3fmevKertOnrlNLbM/bH9GHjNnv+x/a5e6jfHtZ3U/zumCMICNdRwVul9to/Wy1inwwRpi1pV90NG+TDESIzt0F97tDKlrOebQKazBzZJ3K7GG5uy0rTf3lWMhe9WYudb4WqdhjmPNaNlL7tmQNiUZ+8NY17XPE3mm95OyPbW8JN2ydfxy41JruYuljqde0nUpnKLbFWXr5KLbDemHJWuwAITihWd7OilmvdC/28y7uIc0qnU+qaUVrymW5fq/BODtAD61Xv8ogPcergtBEARHRy9NTURyEfkygBcBfBbA3wE4n9LrQ/gzAN5wY7oYBEHQn16DWkqpSSm9GcC9AN4K4Hv67kBEHhKRJ0TkiZancYMgCI6Ya5r9TCmdB/B5AG8DcEauGFTuBfDsVb7zSErp/pTS/ZlT5CEIguAo6ZwoEJE7AVQppfMiMgXwTgC/hNXg9j4AHwfwAIDHuncnSAd2mU/07nd3bYBuTckFWzKsIqenPyehoQgbZSkZHycwLKxo2SzI9NtS5Wuq2M5V3wGgJqOwqYRtKpQ7JDYB8+e03CMpIptJU0cFL69h4XOWSIB2zLcc0M+74UD6xIHWsMkZTbA5H4pbXYug88HJGNNSi9wAMC61qXVZ6QDughItZoW9P9pEZmwyMJdkzj011clOAWCXkkKeu0AVqWgybnJKT3it96yWZlT5KtHEAVf0AoCCzuqJqZ7AaCj5a3JMwDyps3duz2zj0Wf28x4Aj4pIjtWT3SdTSp8Rka8B+LiI/EcAXwLwsV57DIIguIF0Dmoppb8C8BZn/Tex0teCIAiODSFyBUEwKLYa0C6ZkC5AJs/GBpLXpF8kGoe5uvrISWhoq5GQxkYBuZMxFY0AUJHxcUFFYoTe/zlp4nrt5n4Rnk+UdYZrjm+H1ZDY+Cgm+Li7kjwn/OQAfw6SXn1JL+YU0D6iIiENF90BUJOWyW22bKwdOwlAKypoMtHaFRurOckCABQT1sx04sSGKrR7Fe5JqsLpU3fq75AWzLodAIxLKiJEmmNF59ALaC8LKqKT6+sg0NfJu0+ruda/xqSpLWekjzlFdTiRZF/iSS0IgkERg1oQBIMiBrUgCAbFVjW1tmkx3zvo7yKPkeN3MZYq0iLm+9ovxsHYADCeaE1gn4JrF3uX9D7G1kM0mp5SyxW97zdzCqz3dKiuiGxTVNfLEmm+tHmDHsn3TJA3nUOv0EjGhUQ4+DzrSAoIoDXFizcHtBdO8V4umsOFmJnRSScR6VzrStW8I9jaCYxpyOvIRbil1m14GiMXH9knjbmtdD+5KDcA7J7WvrMd2qYi7xt70ACgoGItrHUW0L+n5OjHifo+n+nzs+BCLM7Ppa1DUwuCIIhBLQiCYRGDWhAEg2KrmlpCQn3AjDMZaU8NF+8AgJZ9NaQrINeWgwYuAAAfwElEQVSfj5xixjXHi5LulOes5TiJ8xY60WRGvhohDYULXgDoDNQ0H3tJIjlOk/Uu0q48Fc+0wfGk5G3yGunK37cz0T6tfS/2k1rJqBKLlPpaJte3xHGr+tOWjq0gzxUA1CPtmSqSvpYZx7k6xa75FNWky3Hcb+PolHmh22WvV07xoslJ3rlPBY93d7XGVo70Pvb3z8FCvjRTIUcvej5G62WjxJLkBS3c3J26jfmrl+xGDvGkFgTBoIhBLQiCQRGDWhAEgyIGtSAIBsV2K7QLlMFy2XCyPcd821HJmZPtLTnAGcCIgnybuRY29yg4vW31MmCNgGzyLGgfS2+ioKPSk6ke7nlvR/qSsSHTM4aaNng/XYknnQkLrmQkZKSe7OiJguW+TfDH55CF8hOndBsXX7FVvhJPFPA5pkkfLwicq3pNT53RbdAJaXgiBUBD13t+nkzhHccKAAWZXBNtk5mqTVZd54pT+2ROH421+biqbNWzep+SQHJSSJ7kcHOI8mTU5t+xlHZyTuwp6kU8qQVBMChiUAuCYFDEoBYEwaDYbpJIyVAe0J6qudY38sKpWk3JBrnMXqrJ1OiM01JqM2FGL/RVpXWILLOnJSdTr1A/MkqsmGVOMkInCaZqk7ruVeDOqaBJOdXHVgtpio5BU2gdVxdP5GAtnSIhdWKTr/7OxfMvUze8jJfUL/6cTNFmGUBOCQ2Fzo+QDsUJQQFgNNJG0BNUjIR1uvm+NmIDQLsgjZUKrTSsXTm6HEiXm1Iweksm19oJ+K5afY+xljXlojLJ/l6apb4f2HxsNFZXU+tIZmqyeXr36eFKtMeTWhAEgyIGtSAIBkUMakEQDIqtamqZCMYHivxWe9q75GlZHDzcUMEG1rq8JIFVq/1wIyr4yl6d1imswcG1XJyEdQavwKuxoXUUFfYqWjTsl6Miwpx40XiKnGbZ68V/6vKR1dS4+CwXEuHiJJmjl2bQ14ELi+xf1AHMpVNUZ1Rq31Vh+qoPdjSxSSJPnb2L2qAkiKR/XTznBLTXWmdrRuRbqzbrqban9liWC0qI6SXvpOVqzkW29XXamThFhug7TbVZP2U9edW5rqLStOgVmfZqF/UgntSCIBgUMagFQTAoYlALgmBQbFVTa9oGe7MrcZX8Hu1YqtCQ78a8i5OPjZPgAUDi4D3yO7EuVztFc1kjM/ofVY4wiRZh7T2crNFs4BUr4f1Q0QsuiIKl1SoK8p0JnR/W7TzJZDTmhIUcP0qFRhyt8/SZu2iN3hHHU5pkhQCmJ3Wc5u6Zs2p5sa+9kHz+AGA01n6wjO6pS+e0525+3iYrrKmQSkY/LU7w2LbWY8bnfe8iJ3AkLcsr3E0/otTo5YqKuySncPd0lxJ87mmNjX1rNibV+kvZL8d6Yd3aGNQpxam+gmfNNh7xpBYEwaCIQS0IgkERg1oQBIMiBrUgCAbFdgPaIVpAJKG83uekkVboBBsOSy38loWupA4ATaYF2OVS76ekyt+NU3FaQCZgEnVZ+O0qxr5qc3MVq2xiJz0yTi5IiQVb6ns+1QbXVd/oO+xypImD2jMj899Drq5Fswte5aOGrm1Bwelsgs6dKl88l8KFjZb7WuTev2CrJ1UzPZnAEwWXz72klmunqpWZsBpTRaZcn9PZZZvwkuEAdj6FXClrtR+aoCARn6ugLZY2qep0qicKxmf1PVcvdeLJ5FQKy2myrSIDL0/GzZ1+YNQj46lDPKkFQTAoYlALgmBQ9B7URCQXkS+JyGfWy/eJyOMi8pSIfEJEbIBgEATBlrkWTe0DAJ4E8Jpo9UsAfiWl9HER+TUADwL46MYWBNr4ago4OEnh2CzJgeMZG2etiW+x0BpAQxXbC9LUOPEgAMz3uHAI6T2kZbDBFQByCs7PMtKd2LDqmG8bOh8NF14hLe/2u243bcyoevjsktaUEhmNJbN/+9hsaxP6cYC/FRnnVPDm5Cl93jlQmo3GADAng2pNVc0TnR+vAAwncGRjdU3B6KYY0GqtWuJEDKbIjnM+zPFyfgM2kXuaGveDk3lSG2zGBYCCdTnSbRf0nWbuFBlqWJel+5bPYWOPZdFy8Zp+9HpSE5F7AfwLAL++XhYAbwfwqfUmjwJ476F6EARBcIT0ff38VQA/jysF2G4HcD6l16fNngHwBu+LIvKQiDwhIk+YmcwgCIIjpnNQE5EfA/BiSumLh9lBSumRlNL9KaX7OV98EATBUdNHU/tBAD8uIu8GMMFKU/sIgDMiUqyf1u4FekabHnh1TjUFiZfWl1WTJsJKxHKm37uXXrI5fn+nTWoqAFtM7GlpSEMrct3XERVAyTxPFQcXL8mHRFoW61arbTavyOj492Y2+BrUNw7W95SazjUmoF0fW+4UomHj1ZIK77L3yyvwUZPnsKZiPt51YDjIna+T8eRxIgIAibVOSujInjtPczXPGOacdhciMckWTSXmzRo1AMxJdzxxUns/pyPtY9vbf9W0sahZ/6RjMVqfk0TU0bb70PnolFL6UErp3pTSmwD8BIA/SSn9FIDPA3jferMHADx2qB4EQRAcIdfzPvhBAD8rIk9hpbF97Gi6FARBcHiuKUwqpfSnAP50/e9vAnjr0XcpCILg8Gw/9vOAttIIvd8XXiwbJZsjTcAoAk7MpZENbLZGtdg6cYonz9yhlqul9upUpMNktTPTy4VjuzrvHQutzNn/RBrKcm59SBwgmZW6X0JyiFdU2fisTD8okaDjdWPNqKKEjuyPM0k1nf3ySeP4SS8TqSk84534g9t7WiclBWUtmBMpusWuOdbT3A7dAcW2gEnHcmvbbBe6H4tc30M7J3RiyZ1dXfwZAGYXtM5mf7dcdNo0gcQ3Yk9iOjIIgkERg1oQBIMiBrUgCAZFDGpBEAyKrU4UAILiQLBwRYLrxDHfYkKBsTNKJucFwZvdUtUqExesRczxxCZWHE90xaGKEgWaYGuvBBMbITvwqlYb9bhLPPZ2SQHbqSZjpDMxYLthyrxv3t5ts6ONhg2bjgmYJxM6hHJP5OfJhC4DqxfQbqtpcVUn3t6ZSDK7pb73SDxqJk74FLe87FQ9owmt5YySqtLE0mSqfxsAsGz05EK71L+PdkG/l8yZFLjG38vrTR3qW0EQBMeUGNSCIBgUMagFQTAotqyptaibK8HjHBTOxU0AIB9rEaCt9TYta1uedsOmTRMEThrCZZtIMJEOtXNaGw5nr+oq3q7U1SUR9JHLWN/pCnp2ku91YQK4XS1nc1LIThMsbLC5MdeaLImOHkYJHLkSvDk/jpZlZDjqB2tsni7XBd+XXgtdvlkT9O2ZgPn+78j25eq2vA01uSBd20saUOQ6YL8akfZJ16GdeZra4YaneFILgmBQxKAWBMGgiEEtCIJBsXWf2sHA3oIKvs4u2UILUuoX+nyi39U5RXjbOIVmWUMr9GFz8drGJLgDQMWK81wH9ZYTfSzNQms9ACDZZl+WDdh2go056DlxYknzlU6sL4u38HRK1ndMq/rjPj41ulANFdFh75ezGwMXUeFAasCzdnWcxD56aYeprE+xawPrcj3OaVfRaV+3pUWOga8oEQEX/4EtPJNY+6TvjKYnTBvFZIfWvGS28YgntSAIBkUMakEQDIoY1IIgGBTbTRKZZRiPrrw7zxeUFLBwClrQMutf+VQXZ2hq64dijYALvPBey9KeFs5nyYkWSyq80tRWUzPOLqNlcU8czYRlJZalemk115gE0Y1BpU26ZCivIE6H1a0Y6XPqtVGUWmNlvXQxI8+h5+2iznNh5pz2wUVUAFsAxtr2Nnsl1y3rTTqutQ8/p/DNf+2JJm1BIL19tbRa54TiQ8fkWxPRGrVbqKjLZHcV4kktCIJBEYNaEASDIga1IAgGRQxqQRAMiq1OFLRNg72LF19fXpK5MneSRBYjMtuSSCsUKJtPbeXrZk6GXAoMrikoXpIVgkn3RDWn/Y61UVAK+/fCJJIsOVCaqlZz9SkAQoKq9fOyuNo9+cLbWC+ucywmUSK1wUHhPRyrbHrlKuZeckaupg7ZXOU+L6wgnRd039H90ZCh2zM483Ux1dWNOderSMUr7H5or10b2G/wpXRMwnzO7PyMPsdNZSfF5nPdxh1n79Qb7N6uFs+ff8G00XBmzZ7Ek1oQBIMiBrUgCAZFDGpBEAyKrWpqKSVdkIEEoaIcg+HCEA3pTjmZcbOxLZqSSJtqWg6CpzYdPSwjLaatSA8cj2nZanv1gjQz+pvSNlzV2zEB5/p8tG2HYdMxW3bXTOlIPAlPI+sQhBzjLK9hbWu8o4OcjZEWQLXQpldx9J2D1Ev7eUbJSnd2T6vl2eWLapnPOWC1zYbuj16JBoy+tdmd3KtiO1//HlXf24bOEf3GhMqpe/dHQRXppztUnIXauLinK7oDVylO04N4UguCYFDEoBYEwaCIQS0IgkGx5SSRgBwQdDhw1vO78LBbkncpo3dzFE6h2VKvq/Z04Qh+/y9HjrZHQlRLIkrdam2nGHGCO5vQMicNpSb90GgbsLpbZwCzh7A2Y9IGHKINXu4uEmKKTJP/qVrqhAdukgA6H6bIco9jWc71fjgB5m133auWL7zyvGmjpqSgNd/LRh/q1hitonaNhaw9TCFvuwkHkvN+Wft2rH8oSIc7f14neBxTsoKicIai0NSCIAhiUAuCYGD0ev0UkacBXALQAKhTSveLyFkAnwDwJgBPA3h/SuncjelmEARBP65FU/uRlNLBir0PA/hcSunDIvLwevmDXY2wiqKWMhtzOSr1u7ep1UuxoF4xCrZIcTxpOaGkkbkXL7k5Y99yX+t0O+Up0wb7m5paxxRysRYprViRjahIzJxiYUkPEq8AiFnVVUXXEV5sxsuNG3gFgNmH1C60psi6lBeD2pkVkw+FC8YAaCnGcD7TvrT92SXdL0fbq2qtqU53z+o2L5+nbnnxtHQtO/yE4lyXTu8a3Q/edclH2utZTmmZYrTZxwcAywt63ZL04emJM2r51CmKDQVQLa0vsQ/X8/r5HgCPrv/9KID3XkdbQRAER0LfQS0B+GMR+aKIPLRed3dK6bn1v58HcLf3RRF5SESeEJEnvCwLQRAER0nf188fSik9KyJ3AfisiPzNwQ9TSkm8Z+HVZ48AeAQA8lHZJ1gkCILg0PR6UkspPbv+/4sAfh/AWwG8ICL3AMD6/y/eqE4GQRD0pfNJTUROAMhSSpfW//5RAP8BwKcBPADgw+v/P9a9u4RUX3kF5aDvzBHGU+IAbk5gp4VPrwKNsLl2R5sHk2iBtklOdXV6xmxrErlpt60jwNYLHeTczLTps+AqVxwlDaAFVWjn4yUh2FZ99+gIRvfmCdgY2Tlx0GO3HRWXvADnjLMeCt3S3KSTAJQFer52F1/VCQw5aSQANLVuY3JSC+GTE7ep5XqpJ5YAoKkoOUFDSRKFE2J653izGZtPVzm2RvNyrCfnEk3Wjchtu3dBT4LYvdrf6WxPTyR4ySw4SWhf+rx+3g3g99edKgD815TSH4rIFwB8UkQeBPAtAO8/VA+CIAiOkM5BLaX0TQDf56x/BcA7bkSngiAIDktEFARBMCi2XKFdkO9c0Y1STQZNK3eggV5ZUKVnNk7aEuYwQgLHwAO0YuEYRdnUS5raeIeqiXNBEABFrk2MqeSgZwqar73JYkp4Sf0Q0l28qvde8kndDRaz+lhxOkydnge4yyfKBUC8IjJGvKGbiJNoutVMNgfnN0tK+Oi1QH3du/iyWh6Xu7pNSiLpNtxxglKPZ5Ki1Ne6mOjfT17ae+HkSUqSOdP3aUka2+nbrXH2wiv/Ty1Ly+Kmvi77M2vgPXPmO8y6PsSTWhAEgyIGtSAIBkUMakEQDIqtJ4lUsgF5qGouOgwgz9mHRj4t8nLlI8fbstnKhbwlnarqEdBO+kdGSfFSQ4kHAUxO6EIidaUDpSXTfRdTeRZoG72f8Uld0GJxWRewaFsrVFpvV0fmQC+/o63gsRk3Jr5DdzMak9fI5h2zPypjH5uzTcvJOul8ZE7Cg3yk291f6OtvLGZe4gVjMeNEm/Sxo7llVHW75R0XXDTFMtu7oJarpW5jMtHa8HisNTgAKEbau1aRty+n56lU29/+Yn7JrOtDPKkFQTAoYlALgmBQxKAWBMGg2G4x4zahXV7ReDLSITytIiMfjVChYY4HNIVYAAjpXexdkpZj7GwbuWi9KwkXq6U2HG/X6JQuxjJtbtdfyahgsimyAlw+pzWzSaljCoULN1dWq2A9x8bcsl7Ww2TW4SnrlZ6lQx/ziggXTuHpTTv2dCih856R1msKZjuybUP30CjX1zrj54eFd6yb45q52HNyPGZNxUWF9DZzSmaaOz5GUyBI9H7nlDTTvT9ov6bvFCvrnY3lfPtJIoMgCI4dMagFQTAoYlALgmBQxKAWBMGg2K75NgFtdUVQzSh5oycWtiQoNhVXjyJRU/sCV/uBFsuzlkVsMl86weicaJK9uPVC74MnNACgpopDp27TZR3297VhcX7psmmDzciz8zrhsJH8HcOqmYCgrmZk0GQhHbBVqroC2D0jsTG9GtMvT76YJoyB1xiLeXsneac5XhLgeRKoce7U1mSj1P0YTfXEAUpHoF9S9SwS2+ucjOalnbHIc+or/X5qCqT3kpnmtN+GNpnNtDnXMzQn0f3IqAJ7ywkwvaSqXtB/D+JJLQiCQRGDWhAEgyIGtSAIBsXWA9oPvqCnmt+7rXbDVcubiguPaDz/pjFPUqLJ02e0gXU51wnuAKBttWmxJW0PDRV3mTqFJMgIm1GSyGZfa25eMseCKrTX+6z/9THO6kXWENmsnLlq52Y7bZ9Ek62p4k792LiHdRtU+Ruk/3E3OPEAACSu1UK6JSfiTCwywSZBnOzqIO/pRGtqi4tWL02F3qYmk3Tb6Psjz2yBoJRpk+tirvdTkDbcsLYFICs4sYI+toqKxmRwkiaQqZf185SxRm3vj5wKEfUlntSCIBgUMagFQTAoYlALgmBQbF1TO2gjSkt+F7djLAfCsqbGelDmaDf5RCdSLEh3WFICu8zxmKWatJqMdBYOkve0LNIvLp17RS1X5GPzimKwD4v1QpMU0dHlTCy6o2d0fW79Xh1FQtziLaypdSWN7C6awoHlJpklC2gAmiV5DMntl3EB7cxqPafvuEMtX9rTSSIvvqKDwFlPBgBptS9rydoVJYBAZnXbnPRi9hjydUiOCG00w3Kzb9FLvJC32jCakcculXSdnNsn9SrEbYkntSAIBkUMakEQDIoY1IIgGBTb1dREJyRsZlpDaJze7FJhkSzTxUvSiPSO0hmn6d2ci2LUiXUIR1NjDY2LcYxY+7PJGWdJJ72b72nNZHRKH6spoAyAuoGcYuo4xjJ34jZZy6o5npbiRZOTvLOzSorpuqOpGY2MzzFv7yR4pH6wt6ma6XNcJ61bAkBJBXFG5Bc7cTsV1a2thnTpZa2PXjyvl8ux3kfm+fxa7TtjXxpqrVPlieJJYYvG8P1g9S/bj4oTTQolvCQ/aeMU92lqimNl+yQnp3Tls16pRQ3xpBYEwaCIQS0IgkERg1oQBIMiBrUgCAbFVicKRATZASG3LbUw7InrcxJli7EW09uCxFVHXG+p3VFO+yVTbDmamjb2ZrqKE1fPLkbaCFnPrSBtTb5UcYeqWJmgeQCp5kpYJOqTaFs7wcbCpleuptVuNhoDTtJHc9q7A9oTSNSmoOeMJjmapTWsti0ZZxMZRanN1jG98jmb0MTB/kVK3rlnqxxx4lE+xxklmiyceyzxTBmdY07EIHWPBKAm4Sn105sUo+80VD1deNLMCYpPBa1jI23GxmvTxGHnCeJJLQiCYRGDWhAEg6LXoCYiZ0TkUyLyNyLypIi8TUTOishnReQb6//f1t1SEATBjaWvpvYRAH+YUnqfiIwA7AD4BQCfSyl9WEQeBvAwgA9ubiYp/ULGWv8YiQ3QXS60NsU6UzbS43LlJb3LqIAFyQwN+P3fKUZBRS5MQktT1dr+vVhSYY2MjKJL0v7afavLNc3mytZ9AutNKDpvw7qkG4y+2XzLgdOsBwFAS5oQ98MEuDuB9S0lOIDoc3zqrDbO7u1pfQwARhN9300nWu+6dEkbaeGYTeuF7kfO54x0y8zRKZGRyZUDx5O+P1LdozAJGaknU5288tTZv2e+Mq+1Of3yOV3cp6HK6VnuHAsnhaCfCxc7OrSA5tD5pCYipwH8MICPAUBKaZlSOg/gPQAeXW/2KID3HlmvgiAIDkmf18/7ALwE4DdF5Esi8usicgLA3Sml59bbPA/gbu/LIvKQiDwhIk+0ThrkIAiCo6TPoFYA+H4AH00pvQXAHlavmq+TVu8J7oiVUnokpXR/Sun+zI0hDIIgODr6aGrPAHgmpfT4evlTWA1qL4jIPSml50TkHgAvXrWFNSkBzYHEkNmEtK7a+tRyLiIsm4OehQvgAmgXVBCZ2uDirTtn7zRtZDtn1PJsppP+7b3ysm7TKSIDLj5BQgP7n7w/AZnxFXX9oehOrGir15A+5hR3BgUkc7ESsG8rs5oaRzk3FWmO9HkxtgHclWjdkX1pxUTrlneeuMe2QV439rLtnNBzYPvQxXwBoG10EHiZ6/uF9bHFwnrd8oL00sTX2lwo0wZTkBZ85q43qWX2+QFAta/vbQ60z1mD5eB0GIudSUbAPj7vLa41SWT70fmkllJ6HsC3ReS716veAeBrAD4N4IH1ugcAPHaoHgRBEBwhfWc//y2A317PfH4TwL/GakD8pIg8COBbAN5/Y7oYBEHQn16DWkrpywDudz56x9F2JwiC4PrYbpLINqE9EL/HieK4ACoAtDMq2MqFRUi7Kke2KAYXJ5lTQYui0L6k5LzKNyyRkWaUT3QCv3JsPXflVGtCqdVv/5cuaFnSCcszSRGF+srqV3I0RnED7TbgbC+kQwrF9rUU59rQOQeARG3klJxxQucwG1ldriTf3vLyRbW8d17H7OZnz5o2Fnva71UVHD9LGqPzs8kn7DGjwsQL0qmcS9AsrS/xIEK6FBclAmzhldN3fqfeB8VSX3jladNGS9qmkOeOE1FmrZdUlVbUmwsmefGjvE1fIkwqCIJBEYNaEASDIga1IAgGRQxqQRAMiq1XaD8YLE3xua6ozUnvWCzloOfx7inTBgudTUsCNFW6rqgSDgC0i81J7fKdXbU82dXLAFBRwDpXizp5Rlf5Xs5JXAbQkqDaLrlNElcdkb9rmsAGyTttUFaAlg27iSd0rKg93tUJPzlpwM4JfS33Z9awyqbOhvqa0fm6dMEaZzMyBjecNIH6VYx0UDhgTaxZoSeOpjSR0Djm22ZJVc4qPYFhq9zbZ5LJydt1Xye6rxdefJr6YYPiM07oyL85Fv2dRAPctTzX1z/xb8H57XMiir7Ek1oQBIMiBrUgCAZFDGpBEAwKMYn4buTORF7CKqTqDgAvd2x+HLhV+gncOn29VfoJ3Dp9vVX6CVxfX78zpWSzTRBbHdRe36nIEyklL+zqWHGr9BO4dfp6q/QTuHX6eqv0E9hOX+P1MwiCQRGDWhAEg+JmDWqP3KT9Xiu3Sj+BW6evt0o/gVunr7dKP4Et9PWmaGpBEAQ3inj9DIJgUMSgFgTBoNjqoCYi7xKRr4vIU+sCyMcGEfkNEXlRRL5yYN2xq0IvIm8Ukc+LyNdE5Ksi8oFj3NeJiPy5iPzluq+/uF5/n4g8vr4PPrFOE3/TEZF8XQbyM+vl49rPp0Xkr0XkyyLyxHrdcbz+Z0TkUyLyNyLypIi8bRv93NqgJiI5gP8M4J8D+F4APyki37ut/ffgtwC8i9Y9jFUV+u8C8DlQacCbRA3g51JK3wvgBwD89Po8Hse+LgC8PaX0fQDeDOBdIvIDAH4JwK+klP4hgHMAHryJfTzIBwA8eWD5uPYTAH4kpfTmA56v43j9PwLgD1NK3wPg+7A6tze+nymlrfwH4G0A/ujA8ocAfGhb++/ZxzcB+MqB5a8DuGf973sAfP1m99Hp82MA3nnc+wpgB8BfAPgnWDnKC+++uIn9u3f9I3s7gM9glazk2PVz3ZenAdxB647V9QdwGsD/wXoycpv93Obr5xsAfPvA8jPrdceZXlXobxYi8iYAbwHwOI5pX9evdF/Gqi7sZwH8HYDzKb1eCeK43Ae/CuDncaXMw+04nv0EVtmj/lhEvigiD63XHbfrfx+AlwD85vqV/tdF5AS20M+YKOhJWv1pOTb+FxE5CeB3AfxMSklVGzlOfU0pNSmlN2P1JPRWAN9zk7tkEJEfA/BiSumLN7svPfmhlNL3YyXl/LSI/PDBD4/J9S8AfD+Aj6aU3gJgD/SqeaP6uc1B7VkAbzywfO963XHmhXX1efStQr8NRKTEakD77ZTS761XH8u+vkZK6TyAz2P1GndGRF7LAHgc7oMfBPDjIvI0gI9j9Qr6ERy/fgIAUkrPrv//IoDfx+qPxXG7/s8AeCal9Ph6+VNYDXI3vJ/bHNS+AOC71jNKIwA/gVWV9+PMsatCLyIC4GMAnkwp/fKBj45jX+8UkTPrf0+x0v6exGpwe996s5ve15TSh1JK96aU3oTVffknKaWfwjHrJwCIyAkR2X3t3wB+FMBXcMyuf0rpeQDfFpHvXq96B4CvYRv93LJ4+G4Af4uVrvLvbqaQ6fTtdwA8B6DC6q/Mg1jpKp8D8A0A/xPA2WPQzx/C6pH9rwB8ef3fu49pX/8xgC+t+/oVAP9+vf4fAPhzAE8B+G8Axje7rwf6/E8BfOa49nPdp79c//fV135Hx/T6vxnAE+vr/98B3LaNfkaYVBAEgyImCoIgGBQxqAVBMChiUAuCYFDEoBYEwaCIQS0IgkERg1oQBIMiBrUgCAbF/wdaz3x1XyG4kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(18, 5)) \n",
    "\n",
    "axs.imshow(im_batch[0,:,:,1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to the CNN Development!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "# The GPU id to use\n",
    "# Patrick \"0\"\n",
    "# Feroze  \"1\"\n",
    "# Yousuf  \"2\"\n",
    "# Diego   \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:2',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do other imports now...\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 7)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "num_classes = len(np.unique(label_image))\n",
    "epochs = 50\n",
    "\n",
    "# input image dimensions\n",
    "tile_side = 64\n",
    "img_rows, img_cols = tile_side, tile_side\n",
    "img_bands = landsat_datasets[0].count - 1\n",
    "\n",
    "weight_decay = 0.01\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the model\n",
    "\n",
    "This is just a simple CNN model but it should be able to perform well above random when predicting landcover types if everything is correct thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 62, 62, 64)        4096      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 60, 60, 22)        12694     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 30, 30, 22)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 30, 30, 22)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 30, 30, 64)        12736     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 23)                11799     \n",
      "=================================================================\n",
      "Total params: 2,659,501\n",
      "Trainable params: 2,659,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(tile_side, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(22, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 64, 64, 32)        2048      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 23)                188439    \n",
      "=================================================================\n",
      "Total params: 478,391\n",
      "Trainable params: 477,495\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=input_shape))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the train/validation pixel locations to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_px, val_px = gen_pixel_locations(image_datasets=landsat_datasets, \n",
    "                                       train_count=50000, val_count=500, tile_size=tile_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up the remaining model hyperparameters and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#sgd = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[25,22,60,60] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_22/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_3/SGD/gradients/conv2d_22/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_21/Relu, conv2d_22/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_3/mul/_687}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_999_loss_3/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ec8afa82d432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_px\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandsat_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_side\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_side\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=len(val_px) // batch_size)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25,22,60,60] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_22/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_3/SGD/gradients/conv2d_22/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_21/Relu, conv2d_22/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_3/mul/_687}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_999_loss_3/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator(landsat_datasets, label_dataset, tile_side, tile_side, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_datasets, label_dataset, tile_side, tile_side, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's evaluate the Model\n",
    "\n",
    "We'll just generate 500 test pixels to evaluate it on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a built in evaluate_generator function and because we told it above to use accuracy as a metric this function automatically outputs categorical accuracy which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(generator=tile_generator(landsat_datasets, label_dataset, 11, 11, val_px, batch_size), \n",
    "                        steps=len(val_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this simple model we're getting 37% accuracy across 23 classes which is well above the random accuracy which would be around 4% (aka 1/23). That means we're in business!\n",
    "\n",
    "### Evaluating the model ourselves\n",
    "\n",
    "If we wanted to run this evaluation and take a look at specific predictions and labels we can do that below (albeit more inefficiently) just to get an intuitive understanding of what is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(test_px):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = np.zeros(pred_index.shape)\n",
    "correct_predictions[label_index == pred_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(correct_predictions) / len(test_px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now maybe more informatively let's build a confusion matrix using the scikit-learn function.\n",
    "\n",
    "Read the docs here and make this more informative by following some of their examples: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn ML Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_generator(image_datasets, label_dataset, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "  \n",
    "    image_batch = np.zeros((batch_size, band_count))\n",
    "    label_batch = np.zeros((batch_size, class_count))\n",
    "    b = 0\n",
    "    while b < batch_size:\n",
    "        # if we're at the end  of the data just restart\n",
    "        if i >= len(pixel_locations):\n",
    "            i=0\n",
    "        c, r = pixel_locations[i][0]\n",
    "        dataset_index = pixel_locations[i][1]\n",
    "        i += 1\n",
    "        tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c, r, 1, 1))\n",
    "        if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "            pass\n",
    "        elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "            # we don't want tiles containing nan or -9999 this comes from edges\n",
    "            # this also takes a while and is inefficient\n",
    "            pass\n",
    "        else:\n",
    "            tile = adjust_band(tile)\n",
    "            # reshape from raster format to image format\n",
    "            reshaped_tile = reshape_as_image(tile)\n",
    "\n",
    "            # find gps of that pixel within the image\n",
    "            (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "            # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "            inProj = Proj(image_datasets[dataset_index].crs)\n",
    "            if inProj != outProj:\n",
    "                x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "            # reference gps in label_image\n",
    "            row, col = label_dataset.index(x,y)\n",
    "\n",
    "            # find label\n",
    "            label = label_image[:, row, col]\n",
    "            # if this label is part of the unclassified area then ignore\n",
    "            if label == 0 or np.isnan(label).any() == True:\n",
    "                pass\n",
    "            else:\n",
    "                # add label to the batch in a one hot encoding style\n",
    "                label_batch[b][label] = 1\n",
    "                image_batch[b] = reshaped_tile\n",
    "                b += 1\n",
    "    return (image_batch, label_batch)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height * tile_width * band_count))\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            elif tile.shape != (band_count, tile_width, tile_height):\n",
    "                print('wrong shape')\n",
    "                # somehow we're randomly getting tiles without the correct dimensions\n",
    "                # I assume it is when the tiles are on the edge\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile)\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile.flatten()\n",
    "                    b += 1\n",
    "        return (image_batch, label_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_px, val_px = gen_pixel_locations([landsat_dataset], 50000, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "sk_im_batch, sk_label_batch = pixel_generator([landsat_dataset], label_dataset, train_px, batch_size)\n",
    "print(sk_im_batch.shape, sk_label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "sk_im_batch_val, sk_label_batch_val = pixel_generator([landsat_dataset], label_dataset, val_px, batch_size)\n",
    "print(sk_im_batch_val.shape, sk_label_batch_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 50\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "clf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(sk_label_batch_val, axis=1)\n",
    "\n",
    "clf.score(sk_im_batch_val, label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = clf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(sk_label_batch_val, axis=1), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_index, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize our model with 500 trees\n",
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(sk_im_batch_val, label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for b, imp in zip(bands, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = rf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_index, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training data that contains a 3x3 tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "sk_im_batch, sk_label_batch = sk_tile_generator([landsat_dataset], label_dataset, 3, 3, train_px, batch_size)\n",
    "print(sk_im_batch.shape, sk_label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "sk_im_batch_val, sk_label_batch_val = sk_tile_generator([landsat_dataset], label_dataset, 3, 3, val_px, batch_size)\n",
    "print(sk_im_batch_val.shape, sk_label_batch_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(sk_label_batch_val, axis=1)\n",
    "\n",
    "rf.score(sk_im_batch_val, label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for b, imp in zip(bands, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = rf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(gamma='scale')\n",
    "svm_clf.fit(sk_im_batch, np.argmax(sk_label_batch, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.score(sk_im_batch_val, np.argmax(sk_label_batch_val, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = svm_clf.predict(sk_im_batch_val)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                      class_dict=class_names,\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a balanced percent of each class\n",
    "# look at the classes that are consistently mis classified\n",
    "# think about adding NDVI or taking out bands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "input_shape = (64,64,3)\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 2, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 23)                23575     \n",
      "=================================================================\n",
      "Total params: 16,836,439\n",
      "Trainable params: 9,201,175\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'landsat_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-701be77772bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mvggmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m vggmodel.fit_generator(generator=tile_generator([landsat_dataset], label_dataset, 44, 64, train_px, batch_size), \n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_px\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlandsat_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'landsat_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 2, 2, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 23)                23575     \n",
      "=================================================================\n",
      "Total params: 16,836,439\n",
      "Trainable params: 9,201,175\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggmodel = Sequential()\n",
    "vggmodel.add(vgg_conv)\n",
    "vggmodel.add(Flatten())\n",
    "vggmodel.add(Dense(1024, activation='relu'))\n",
    "vggmodel.add(Dropout(0.5))\n",
    "vggmodel.add(Dense(num_classes, activation='softmax'))\n",
    "vggmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "\n",
    "vggmodel.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DatasetReader' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a9470ffd1b66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_px\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandsat_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=len(val_px) // batch_size)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e4ad682e6b73>\u001b[0m in \u001b[0;36mtile_generator\u001b[0;34m(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# assuming all images have the same num of bands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mband_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mclass_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_height\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DatasetReader' object does not support indexing"
     ]
    }
   ],
   "source": [
    "vggmodel.fit_generator(generator=tile_generator(landsat_datasets, label_dataset, 64, 64, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_datasets, label_dataset, 64, 64, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
