{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN Framework for Landsat Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from itertools import product\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open(\"/deep_data/data/land_cover_data/landcover_reproject.tif\")\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "tiles = [\"028012\", \"028011\"]\n",
    "\n",
    "landsat_datasets = list(i for i in range(len(tiles)))\n",
    "count = 0\n",
    "for tile in tiles:\n",
    "    landsat_dataset[count] = rasterio.open(\"/deep_data/data/combined/combined\" + tile + \".tif\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection:\n",
      "PROJCS[\"Albers\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378140,298.2569999999957,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]\n",
      "Labels projection:\n",
      "EPSG:32618\n"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_datasets[1].crs # 4326\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator and Prep Fcns\n",
    "\n",
    "This is a typical Keras generator that I've written to allow it to ingest a set of random pixel locations so we can randomly sample throughout the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, band_count))\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile)\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "                middle_pixel_r = r + np.ceil(tile_width/2)\n",
    "                middle_pixel_c = c + np.ceil(tile_height/2)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(middle_pixel_r, middle_pixel_c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of raster datasets and randomly samples `train_count` and `val_count` random pixels from each dataset.\n",
    "\n",
    "It doesn't sample within tile_size / 2 of the edge in order to avoid missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a randomly selected number of pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    total_count = train_count + val_count\n",
    "    for index, image_dataset in enumerate(image_datasets):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(product(rows, columns)), total_count)\n",
    "        \n",
    "        dataset_index = [index] * total_count\n",
    "        \n",
    "        dataset_pixels = list(zip(points, dataset_index))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count]\n",
    "        val_pixels += dataset_pixels[train_count:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the generator and data prep functions\n",
    "\n",
    "Let's make sure all this data prep actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and validation pixel locations\n",
    "train_px, val_px = make_pixel_locations([landsat_dataset], 100, 20, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tile_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-43b56271fb64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtile_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlandsat_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tile_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# print out some image and label batches and check out their shapes\n",
    "im_batch = None\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_generator([landsat_dataset], label_dataset, 11, 11, train_px, 2):\n",
    "    if count > 3:\n",
    "        break\n",
    "    print('Image')\n",
    "    print(im.shape)\n",
    "    print('Label')\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect an image patch\n",
    "\n",
    "While it shouldn't necessarily be recognizable it should look like it has data in it and that it varies somewhat from pixel to pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'im_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d3d9bf0ff9f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'im_batch' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(18, 5)) \n",
    "\n",
    "axs.imshow(im_batch[0,:,:,1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to the CNN Development!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 7)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "#num_classes = len(np.unique(label_image))\n",
    "epochs = 100\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 11, 11\n",
    "img_bands = landsat_dataset.count\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the model\n",
    "\n",
    "This is just a simple CNN model but it should be able to perform well above random when predicting landcover types if everything is correct thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 9, 9, 11)          704       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 22)          2200      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 44)          8756      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 128)         50816     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 44)                5676      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 23)                1035      \n",
      "=================================================================\n",
      "Total params: 69,187\n",
      "Trainable params: 69,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(11, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(22, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(44, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(44, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the train/validation pixel locations to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_px, val_px = make_pixel_locations(image_datasets=[landsat_dataset], \n",
    "                                       train_count=10000, val_count=300, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up the remaining model hyperparameters and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 1.9665 - acc: 0.3523 - val_loss: 1.7410 - val_acc: 0.3867\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.8697 - acc: 0.3704 - val_loss: 1.7511 - val_acc: 0.3567\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.8244 - acc: 0.3865 - val_loss: 1.7662 - val_acc: 0.3667\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.7902 - acc: 0.3948 - val_loss: 1.7243 - val_acc: 0.3600\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7616 - acc: 0.4008 - val_loss: 1.7139 - val_acc: 0.3433\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.7297 - acc: 0.4115 - val_loss: 1.7306 - val_acc: 0.3100\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7097 - acc: 0.4188 - val_loss: 1.7447 - val_acc: 0.3567\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6902 - acc: 0.4232 - val_loss: 1.8117 - val_acc: 0.3200\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.6790 - acc: 0.4298 - val_loss: 1.7863 - val_acc: 0.3133\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.6537 - acc: 0.4355 - val_loss: 1.8007 - val_acc: 0.3167\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6399 - acc: 0.4418 - val_loss: 1.8899 - val_acc: 0.2733\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6118 - acc: 0.4511 - val_loss: 1.9231 - val_acc: 0.3000\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.5991 - acc: 0.4549 - val_loss: 1.7722 - val_acc: 0.3367\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5909 - acc: 0.4623 - val_loss: 1.8029 - val_acc: 0.3567\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5634 - acc: 0.4694 - val_loss: 1.8831 - val_acc: 0.3267\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5190 - acc: 0.4827 - val_loss: 1.8415 - val_acc: 0.3233\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.5063 - acc: 0.4865 - val_loss: 1.9149 - val_acc: 0.3100\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.5101 - acc: 0.4879 - val_loss: 1.9817 - val_acc: 0.2567\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.5083 - acc: 0.4884 - val_loss: 1.8856 - val_acc: 0.3100\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4728 - acc: 0.5015 - val_loss: 2.0896 - val_acc: 0.3100\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.4477 - acc: 0.5065 - val_loss: 1.9428 - val_acc: 0.3100\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4234 - acc: 0.5167 - val_loss: 2.0227 - val_acc: 0.3200\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4311 - acc: 0.5169 - val_loss: 2.2195 - val_acc: 0.3233\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4229 - acc: 0.5204 - val_loss: 1.9549 - val_acc: 0.3367\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4208 - acc: 0.5229 - val_loss: 2.0446 - val_acc: 0.3033\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.4426 - acc: 0.5237 - val_loss: 1.8570 - val_acc: 0.3300\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3804 - acc: 0.5413 - val_loss: 2.3077 - val_acc: 0.3067\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3953 - acc: 0.5373 - val_loss: 2.2564 - val_acc: 0.3000\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3813 - acc: 0.5450 - val_loss: 1.9383 - val_acc: 0.3400\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.4324 - acc: 0.5286 - val_loss: 2.2449 - val_acc: 0.3300\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3562 - acc: 0.5471 - val_loss: 2.0643 - val_acc: 0.2933\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3485 - acc: 0.5578 - val_loss: 2.0779 - val_acc: 0.3033\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3120 - acc: 0.5646 - val_loss: 2.2222 - val_acc: 0.3100\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3089 - acc: 0.5685 - val_loss: 2.1239 - val_acc: 0.2967\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3360 - acc: 0.5601 - val_loss: 2.1944 - val_acc: 0.3233\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2759 - acc: 0.5760 - val_loss: 2.2455 - val_acc: 0.3167\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3015 - acc: 0.5668 - val_loss: 2.3266 - val_acc: 0.2733\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3682 - acc: 0.5531 - val_loss: 2.1788 - val_acc: 0.2800\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3247 - acc: 0.5640 - val_loss: 2.3444 - val_acc: 0.3000\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2700 - acc: 0.5833 - val_loss: 2.2276 - val_acc: 0.2967\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2606 - acc: 0.5825 - val_loss: 2.2253 - val_acc: 0.3000\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2267 - acc: 0.5924 - val_loss: 2.3397 - val_acc: 0.2933\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2496 - acc: 0.5929 - val_loss: 2.4980 - val_acc: 0.2867\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2245 - acc: 0.5949 - val_loss: 2.3518 - val_acc: 0.3433\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2448 - acc: 0.5941 - val_loss: 2.0161 - val_acc: 0.3500\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2196 - acc: 0.5985 - val_loss: 2.2962 - val_acc: 0.3033\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3293 - acc: 0.5702 - val_loss: 2.4987 - val_acc: 0.3133\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2246 - acc: 0.6009 - val_loss: 2.1797 - val_acc: 0.3467\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2139 - acc: 0.6064 - val_loss: 2.1835 - val_acc: 0.3200\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2238 - acc: 0.6031 - val_loss: 2.1989 - val_acc: 0.2567\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2549 - acc: 0.5931 - val_loss: 2.0613 - val_acc: 0.3500\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1918 - acc: 0.6099 - val_loss: 1.9630 - val_acc: 0.3300\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2002 - acc: 0.6100 - val_loss: 2.0396 - val_acc: 0.3133\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2016 - acc: 0.6097 - val_loss: 2.2566 - val_acc: 0.3333\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1866 - acc: 0.6126 - val_loss: 2.1918 - val_acc: 0.3067\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2163 - acc: 0.6153 - val_loss: 2.3162 - val_acc: 0.2933\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2797 - acc: 0.5933 - val_loss: 2.3134 - val_acc: 0.3300\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2129 - acc: 0.6066 - val_loss: 2.3031 - val_acc: 0.2733\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2221 - acc: 0.6113 - val_loss: 2.3462 - val_acc: 0.3033\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1852 - acc: 0.6150 - val_loss: 2.2605 - val_acc: 0.3300\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2131 - acc: 0.6088 - val_loss: 2.6171 - val_acc: 0.3067\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 2.0336 - acc: 0.4296 - val_loss: 1.9924 - val_acc: 0.3233\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.8741 - acc: 0.4209 - val_loss: 2.1263 - val_acc: 0.2933\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6627 - acc: 0.4776 - val_loss: 2.2062 - val_acc: 0.3467\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.4355 - acc: 0.5448 - val_loss: 2.1835 - val_acc: 0.3600\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.3395 - acc: 0.5728 - val_loss: 2.2894 - val_acc: 0.2833\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.2894 - acc: 0.5879 - val_loss: 2.4162 - val_acc: 0.3100\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2696 - acc: 0.5975 - val_loss: 2.6825 - val_acc: 0.2467\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2860 - acc: 0.5913 - val_loss: 2.4323 - val_acc: 0.2267\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2542 - acc: 0.5967 - val_loss: 2.4239 - val_acc: 0.2933\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1960 - acc: 0.6162 - val_loss: 2.1764 - val_acc: 0.3733\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2227 - acc: 0.6107 - val_loss: 2.5960 - val_acc: 0.2933\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2184 - acc: 0.6105 - val_loss: 2.5864 - val_acc: 0.2900\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.3317 - acc: 0.5868 - val_loss: 2.2913 - val_acc: 0.3367\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2217 - acc: 0.6097 - val_loss: 2.9515 - val_acc: 0.2533\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1880 - acc: 0.6170 - val_loss: 2.7568 - val_acc: 0.3200\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2313 - acc: 0.6057 - val_loss: 2.2282 - val_acc: 0.3500\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1570 - acc: 0.6267 - val_loss: 2.7305 - val_acc: 0.2733\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1725 - acc: 0.6293 - val_loss: 2.8362 - val_acc: 0.2767\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1547 - acc: 0.6290 - val_loss: 2.7469 - val_acc: 0.2867\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1847 - acc: 0.6259 - val_loss: 2.5515 - val_acc: 0.2600\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1470 - acc: 0.6335 - val_loss: 2.4428 - val_acc: 0.3000\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2097 - acc: 0.6142 - val_loss: 2.5620 - val_acc: 0.3333\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.2240 - acc: 0.6235 - val_loss: 2.5495 - val_acc: 0.2800\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1596 - acc: 0.6306 - val_loss: 2.6882 - val_acc: 0.2533\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1975 - acc: 0.6250 - val_loss: 2.8829 - val_acc: 0.2433\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1362 - acc: 0.6374 - val_loss: 2.4614 - val_acc: 0.2833\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1352 - acc: 0.6392 - val_loss: 2.4352 - val_acc: 0.2700\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1436 - acc: 0.6374 - val_loss: 2.5146 - val_acc: 0.2833\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1227 - acc: 0.6412 - val_loss: 2.6880 - val_acc: 0.2667\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1642 - acc: 0.6338 - val_loss: 2.7989 - val_acc: 0.2600\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1330 - acc: 0.6474 - val_loss: 2.7516 - val_acc: 0.2767\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1623 - acc: 0.6315 - val_loss: 2.5551 - val_acc: 0.3100\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.1063 - acc: 0.6489 - val_loss: 2.7606 - val_acc: 0.2700\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.0875 - acc: 0.6541 - val_loss: 2.5889 - val_acc: 0.2200\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.1425 - acc: 0.6450 - val_loss: 2.7921 - val_acc: 0.2900\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.0959 - acc: 0.6536 - val_loss: 2.8151 - val_acc: 0.2700\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.0528 - acc: 0.6649 - val_loss: 2.4596 - val_acc: 0.3000\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.0975 - acc: 0.6549 - val_loss: 2.6209 - val_acc: 0.2567\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.0740 - acc: 0.6601 - val_loss: 2.7128 - val_acc: 0.3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffafa1dacc0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator([landsat_dataset], label_dataset, 11, 11, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's evaluate the Model\n",
    "\n",
    "We'll just generate 500 test pixels to evaluate it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'landsat_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1caaadcb9f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_px\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_px\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pixel_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlandsat_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtile_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'landsat_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_px, val_px = make_pixel_locations([landsat_dataset], train_count=500, val_count=0, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a built in evaluate_generator function and because we told it above to use accuracy as a metric this function automatically outputs categorical accuracy which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 8s 376ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9832591772079469, 0.3760000020265579]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this simple model we're getting 37% accuracy across 23 classes which is well above the random accuracy which would be around 4% (aka 1/23). That means we're in business!\n",
    "\n",
    "### Evaluating the model ourselves\n",
    "\n",
    "If we wanted to run this evaluation and take a look at specific predictions and labels we can do that below (albeit more inefficiently) just to get an intuitive understanding of what is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 7s 373ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(test_px):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 23)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 14,  5, 13, 13, 20,  8, 21, 13, 12])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = np.zeros(pred_index.shape)\n",
    "correct_predictions[label_index == pred_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1.])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.376"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(correct_predictions) / len(test_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.5005555e-05, 2.5345067e-05, 6.9519803e-03, 9.5544951e-03,\n",
       "        1.7500592e-02, 4.5062839e-03, 7.9424324e-05, 5.6560391e-05,\n",
       "        4.5279139e-03, 8.9906462e-05, 8.1206148e-04, 4.5664867e-05,\n",
       "        5.2689546e-04, 2.0000549e-02, 8.8348606e-04, 2.6076639e-02,\n",
       "        3.8386403e-05, 7.5204247e-05, 5.2162164e-01, 3.7951332e-02,\n",
       "        6.4366475e-02, 2.8416619e-01, 6.7970672e-05],\n",
       "       [9.6968334e-04, 6.7671924e-04, 1.0775626e-02, 1.2592370e-02,\n",
       "        1.2026070e-02, 1.2465723e-02, 3.3110134e-02, 1.2139344e-03,\n",
       "        2.5713392e-02, 2.2136976e-03, 3.7321962e-02, 2.8844578e-03,\n",
       "        6.2341351e-02, 1.9656718e-01, 1.5323413e-02, 1.5057945e-01,\n",
       "        4.1870007e-04, 6.7505526e-04, 8.9561611e-02, 1.7911386e-02,\n",
       "        2.0733811e-02, 2.9347154e-01, 4.5277303e-04],\n",
       "       [2.0889305e-04, 1.2599016e-04, 9.1587409e-04, 8.8419262e-03,\n",
       "        5.2371044e-02, 3.6170930e-02, 3.1249449e-01, 5.3113926e-04,\n",
       "        1.3841026e-01, 6.5567750e-03, 1.9548453e-01, 3.2652885e-02,\n",
       "        9.5884621e-02, 5.9320562e-02, 4.4410080e-02, 5.9092906e-03,\n",
       "        1.7554613e-04, 1.5845097e-04, 1.2757443e-03, 1.7050681e-04,\n",
       "        6.1450154e-03, 1.6437772e-03, 1.4168759e-04]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10,  6, 13, 13, 21, 13, 13,  6,\n",
       "       13,  6, 21])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0,  6,  0,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 43,  0,  0,  4,  0,  0,  8,  0,  0,  0,  0,  3],\n",
       "       [ 0,  3,  0,  9,  0,  0,  6,  0,  0,  3,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0, 25,  2,  0,  0,  0,  0, 32,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0, 11,  0,  0,  0,  0,  2],\n",
       "       [ 0,  6,  0,  8,  0,  0,  9,  0,  0, 25,  0,  0,  3,  0,  0],\n",
       "       [ 0,  6,  0, 26,  0,  0, 14,  0,  0, 86,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  5,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  3,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  0,  0,  3,  0,  0,  2,  0, 47]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(label_index, pred_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
