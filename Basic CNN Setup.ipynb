{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN Framework for Landsat Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open('/deep_data/landcover_reproject.tif')\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "landsat_dataset = rasterio.open('/deep_data/LC08_CU_028012_20140814_20171017_C01_V01_SR/combined.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection:\n",
      "PROJCS[\"Albers\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378140,298.2569999999957,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]\n",
      "Labels projection:\n",
      "EPSG:32618\n"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_dataset.crs # 4326\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator and Prep Fcns\n",
    "\n",
    "This is a typical Keras generator that I've written to allow it to ingest a set of random pixel locations so we can randomly sample throughout the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, band_count))\n",
    "        label_batch = np.zeros((batch_size,class_count))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile)\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "                middle_pixel_r = r + np.ceil(tile_width/2)\n",
    "                middle_pixel_c = c + np.ceil(tile_height/2)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(middle_pixel_r, middle_pixel_c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of raster datasets and randomly samples `train_count` and `val_count` random pixels from each dataset.\n",
    "\n",
    "It doesn't sample within tile_size / 2 of the edge in order to avoid missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a randomly selected number of pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    total_count = train_count + val_count\n",
    "    for index, image_dataset in enumerate(image_datasets):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(itertools.product(rows, columns)), total_count)\n",
    "        \n",
    "        dataset_index = [index] * total_count\n",
    "        \n",
    "        dataset_pixels = list(zip(points, dataset_index))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count]\n",
    "        val_pixels += dataset_pixels[train_count:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the generator and data prep functions\n",
    "\n",
    "Let's make sure all this data prep actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and validation pixel locations\n",
    "train_px, val_px = gen_pixel_locations([landsat_dataset], 100, 20, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n",
      "Image\n",
      "(2, 11, 11, 7)\n",
      "Label\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(2, 23)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# print out some image and label batches and check out their shapes\n",
    "im_batch = None\n",
    "\n",
    "count = 0\n",
    "for (im, label) in tile_generator([landsat_dataset], label_dataset, 11, 11, train_px, 2):\n",
    "    if count > 3:\n",
    "        break\n",
    "    print('Image')\n",
    "    print(im.shape)\n",
    "    print('Label')\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    print('----')\n",
    "    count += 1\n",
    "    im_batch = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect an image patch\n",
    "\n",
    "While it shouldn't necessarily be recognizable it should look like it has data in it and that it varies somewhat from pixel to pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb3f084e080>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(18, 5)) \n",
    "\n",
    "axs.imshow(im_batch[0,:,:,1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to the CNN Development!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep some of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 7)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "num_classes = len(np.unique(label_image))\n",
    "epochs = 50\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 11, 11\n",
    "img_bands = landsat_dataset.count\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the model\n",
    "\n",
    "This is just a simple CNN model but it should be able to perform well above random when predicting landcover types if everything is correct thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 9, 9, 11)          704       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 22)          2200      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 22)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3, 3, 22)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 198)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 44)                8756      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 23)                1035      \n",
      "=================================================================\n",
      "Total params: 12,695\n",
      "Trainable params: 12,695\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(11, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(22, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(44, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the train/validation pixel locations to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_px, val_px = gen_pixel_locations(image_datasets=[landsat_dataset], \n",
    "                                       train_count=10000, val_count=100, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up the remaining model hyperparameters and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN THE MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 2.0919 - acc: 0.3349 - val_loss: 1.9130 - val_acc: 0.1800\n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.9168 - acc: 0.3828 - val_loss: 1.9622 - val_acc: 0.1900\n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8929 - acc: 0.3926 - val_loss: 1.9393 - val_acc: 0.2200\n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.8664 - acc: 0.3931 - val_loss: 1.9184 - val_acc: 0.3100\n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8702 - acc: 0.3943 - val_loss: 1.9525 - val_acc: 0.3000\n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8479 - acc: 0.3948 - val_loss: 1.8680 - val_acc: 0.2900\n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8419 - acc: 0.4018 - val_loss: 1.8412 - val_acc: 0.3400\n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8306 - acc: 0.4059 - val_loss: 1.8864 - val_acc: 0.3100\n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8207 - acc: 0.4067 - val_loss: 1.9001 - val_acc: 0.3000\n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8172 - acc: 0.4089 - val_loss: 1.8505 - val_acc: 0.3600\n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8064 - acc: 0.4106 - val_loss: 1.8886 - val_acc: 0.3000\n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.8028 - acc: 0.4125 - val_loss: 1.8429 - val_acc: 0.3200\n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7870 - acc: 0.4162 - val_loss: 1.8815 - val_acc: 0.3500\n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7877 - acc: 0.4139 - val_loss: 1.8839 - val_acc: 0.3100\n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7738 - acc: 0.4180 - val_loss: 1.8645 - val_acc: 0.3300\n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7676 - acc: 0.4191 - val_loss: 1.7912 - val_acc: 0.3400\n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.7647 - acc: 0.4182 - val_loss: 1.8105 - val_acc: 0.3400\n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7633 - acc: 0.4186 - val_loss: 1.9811 - val_acc: 0.3300\n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7545 - acc: 0.4168 - val_loss: 1.9339 - val_acc: 0.2900\n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.7498 - acc: 0.4205 - val_loss: 1.9171 - val_acc: 0.2900\n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7437 - acc: 0.4203 - val_loss: 1.8622 - val_acc: 0.3700\n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.7360 - acc: 0.4222 - val_loss: 1.8363 - val_acc: 0.3400\n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7326 - acc: 0.4235 - val_loss: 1.9647 - val_acc: 0.3100\n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7279 - acc: 0.4250 - val_loss: 1.8875 - val_acc: 0.3200\n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7252 - acc: 0.4215 - val_loss: 1.8370 - val_acc: 0.3500\n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7127 - acc: 0.4266 - val_loss: 1.8420 - val_acc: 0.3500\n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.7110 - acc: 0.4271 - val_loss: 1.8955 - val_acc: 0.3100\n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6938 - acc: 0.4316 - val_loss: 1.8833 - val_acc: 0.3200\n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6995 - acc: 0.4289 - val_loss: 1.8445 - val_acc: 0.2900\n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6921 - acc: 0.4341 - val_loss: 1.7955 - val_acc: 0.3600\n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 1.6929 - acc: 0.4336 - val_loss: 1.8419 - val_acc: 0.3900\n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.6749 - acc: 0.4355 - val_loss: 1.8388 - val_acc: 0.3300\n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6732 - acc: 0.4353 - val_loss: 1.8166 - val_acc: 0.3400\n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6672 - acc: 0.4365 - val_loss: 1.8688 - val_acc: 0.3600\n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6628 - acc: 0.4431 - val_loss: 1.7850 - val_acc: 0.3600\n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6636 - acc: 0.4423 - val_loss: 1.8515 - val_acc: 0.3500\n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6533 - acc: 0.4426 - val_loss: 1.7954 - val_acc: 0.3800\n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6555 - acc: 0.4395 - val_loss: 1.9400 - val_acc: 0.2400\n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 1.6430 - acc: 0.4478 - val_loss: 1.8840 - val_acc: 0.3400\n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6411 - acc: 0.4450 - val_loss: 1.9122 - val_acc: 0.2700\n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6317 - acc: 0.4462 - val_loss: 1.8429 - val_acc: 0.3700\n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6368 - acc: 0.4469 - val_loss: 1.9233 - val_acc: 0.3600\n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6193 - acc: 0.4484 - val_loss: 1.9258 - val_acc: 0.3500\n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6169 - acc: 0.4496 - val_loss: 1.7997 - val_acc: 0.3700\n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6160 - acc: 0.4547 - val_loss: 1.8923 - val_acc: 0.3700\n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6109 - acc: 0.4553 - val_loss: 1.8306 - val_acc: 0.3000\n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6140 - acc: 0.4527 - val_loss: 1.9077 - val_acc: 0.3400\n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.6111 - acc: 0.4482 - val_loss: 1.8985 - val_acc: 0.3700\n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.5986 - acc: 0.4525 - val_loss: 2.0288 - val_acc: 0.3700\n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 1.5955 - acc: 0.4605 - val_loss: 1.9459 - val_acc: 0.3700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2e2477748>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, train_px, batch_size), \n",
    "                    steps_per_epoch=len(train_px) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator([landsat_dataset], label_dataset, 11, 11, val_px, batch_size),\n",
    "                    validation_steps=len(val_px) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's evaluate the Model\n",
    "\n",
    "We'll just generate 500 test pixels to evaluate it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_px, val_px = gen_pixel_locations([landsat_dataset], train_count=500, val_count=0, tile_size=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a built in evaluate_generator function and because we told it above to use accuracy as a metric this function automatically outputs categorical accuracy which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 8s 376ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9832591772079469, 0.3760000020265579]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this simple model we're getting 37% accuracy across 23 classes which is well above the random accuracy which would be around 4% (aka 1/23). That means we're in business!\n",
    "\n",
    "### Evaluating the model ourselves\n",
    "\n",
    "If we wanted to run this evaluation and take a look at specific predictions and labels we can do that below (albeit more inefficiently) just to get an intuitive understanding of what is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 7s 373ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(generator=tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size), \n",
    "                        steps=len(test_px) // batch_size,\n",
    "                         verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = tile_generator([landsat_dataset], label_dataset, 11, 11, test_px, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(test_px):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 23)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 14,  5, 13, 13, 20,  8, 21, 13, 12])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = np.zeros(pred_index.shape)\n",
    "correct_predictions[label_index == pred_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1.])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.376"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(correct_predictions) / len(test_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.5005555e-05, 2.5345067e-05, 6.9519803e-03, 9.5544951e-03,\n",
       "        1.7500592e-02, 4.5062839e-03, 7.9424324e-05, 5.6560391e-05,\n",
       "        4.5279139e-03, 8.9906462e-05, 8.1206148e-04, 4.5664867e-05,\n",
       "        5.2689546e-04, 2.0000549e-02, 8.8348606e-04, 2.6076639e-02,\n",
       "        3.8386403e-05, 7.5204247e-05, 5.2162164e-01, 3.7951332e-02,\n",
       "        6.4366475e-02, 2.8416619e-01, 6.7970672e-05],\n",
       "       [9.6968334e-04, 6.7671924e-04, 1.0775626e-02, 1.2592370e-02,\n",
       "        1.2026070e-02, 1.2465723e-02, 3.3110134e-02, 1.2139344e-03,\n",
       "        2.5713392e-02, 2.2136976e-03, 3.7321962e-02, 2.8844578e-03,\n",
       "        6.2341351e-02, 1.9656718e-01, 1.5323413e-02, 1.5057945e-01,\n",
       "        4.1870007e-04, 6.7505526e-04, 8.9561611e-02, 1.7911386e-02,\n",
       "        2.0733811e-02, 2.9347154e-01, 4.5277303e-04],\n",
       "       [2.0889305e-04, 1.2599016e-04, 9.1587409e-04, 8.8419262e-03,\n",
       "        5.2371044e-02, 3.6170930e-02, 3.1249449e-01, 5.3113926e-04,\n",
       "        1.3841026e-01, 6.5567750e-03, 1.9548453e-01, 3.2652885e-02,\n",
       "        9.5884621e-02, 5.9320562e-02, 4.4410080e-02, 5.9092906e-03,\n",
       "        1.7554613e-04, 1.5845097e-04, 1.2757443e-03, 1.7050681e-04,\n",
       "        6.1450154e-03, 1.6437772e-03, 1.4168759e-04]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 21,  6, 13,  6, 13, 10, 21, 10, 10,  6, 13, 13, 21, 13, 13,  6,\n",
       "       13,  6, 21])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0,  6,  0,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 43,  0,  0,  4,  0,  0,  8,  0,  0,  0,  0,  3],\n",
       "       [ 0,  3,  0,  9,  0,  0,  6,  0,  0,  3,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0, 25,  2,  0,  0,  0,  0, 32,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  2,  0,  0,  0,  0,  0, 11,  0,  0,  0,  0,  2],\n",
       "       [ 0,  6,  0,  8,  0,  0,  9,  0,  0, 25,  0,  0,  3,  0,  0],\n",
       "       [ 0,  6,  0, 26,  0,  0, 14,  0,  0, 86,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  5,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  3],\n",
       "       [ 0,  0,  0,  3,  0,  0,  0,  0,  0, 18,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  0,  0,  3,  0,  0,  2,  0, 47]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(label_index, pred_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
