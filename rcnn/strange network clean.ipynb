{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strange Network Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import keras\n",
    "import random\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Bidirectional\n",
    "from keras.layers import Conv2D, MaxPooling2D, ConvLSTM2D, TimeDistributed, UpSampling2D, Concatenate, LSTM, concatenate\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.optimizers import SGD, Adadelta, Adam\n",
    "from keras import Input\n",
    "from keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import utilities as util\n",
    "import importlib\n",
    "import rnn_tiles\n",
    "import rnn_pixels\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utilities' from '/host/Code/florence_mapping/utilities.py'>"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(rnn_pixels)\n",
    "importlib.reload(rnn_tiles)\n",
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign your specific GPU so we don't overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that GPU and only that GPU visible?\n",
    "\n",
    "Note that it will always say GPU:0 but you should just see one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest Training Labels\n",
    "\n",
    "Note that these are monster files so be careful how you inspect them, typically you only want to use the `rasterio` windows option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_labels = rasterio.open('/deep_data/recurrent_data/NLCD_DATA/landcover/NLCD_2011_Land_Cover_L48_20190424.img')\n",
    "canopy_labels = rasterio.open('/deep_data/recurrent_data/NLCD_DATA/canopy/CONUSCartographic_2_8_16/Cartographic/nlcd2011_usfs_conus_canopy_cartographic.img')\n",
    "class_dict = util.indexed_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the landsat imagery stacked into yearly seasonal tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = {}\n",
    "landsat_datasets = {}\n",
    "tiles['028012'] = ['20110324', '20110612', '20110831', '20111103']\n",
    "tiles['029011'] = ['20110308', '20110425', '20110831', '20111103']\n",
    "tiles['028011'] = ['20110308', '20110628', '20110831', '20111103']\n",
    "#tiles['028012'] = ['20110831']\n",
    "#tiles['029011'] = ['20110831']\n",
    "#tiles['028011'] = ['20110831']\n",
    "for tile_number, dates in tiles.items():\n",
    "    tile_datasets = []\n",
    "    l8_image_paths = []\n",
    "    for date in dates:\n",
    "        l8_image_paths.append('/deep_data/recurrent_data/tile{}/combined/combined{}.tif'.format(tile_number, date))\n",
    "    for fp in l8_image_paths:\n",
    "        tile_datasets.append(rasterio.open(fp))\n",
    "    landsat_datasets[tile_number] = tile_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some hyperparameters for the model and data creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 5\n",
    "tile_list = ['028012', '029011', '028011']\n",
    "class_count = len(class_dict)\n",
    "epochs = 200\n",
    "batch_size = 25\n",
    "clean_pixels_count = 3000000\n",
    "max_count_per_class = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next cell is for making pixels and prepping them for training (balancing)\n",
    "\n",
    "This takes a long time and is unnecessary if you have already saved them to text files, if so, load them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning balanced pixel creation.\n"
     ]
    }
   ],
   "source": [
    "train_px, val_px, test_px = rnn_pixels.balanced_pix_locations(landsat_datasets, lc_labels, \n",
    "        canopy_labels, tile_size, tile_list, clean_pixels_count, class_count, max_count_per_class, \n",
    "                                                              class_dict, buffer_pix=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pixels if necessary\n",
    "\n",
    "TODO probably need to change the text file to shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_px, val_px, test_px = util.read_txt(['train_px.txt', 'val_px.txt', 'test_px.txt'])\n",
    "#train_px = rnn_pixels.delete_bad_tiles(landsat_datasets,lc_labels, canopy_labels, train_px, tile_size)\n",
    "#val_px =  rnn_pixels.delete_bad_tiles(landsat_datasets,lc_labels, canopy_labels, val_px, tile_size)\n",
    "#test_px =  rnn_pixels.delete_bad_tiles(landsat_datasets,lc_labels, canopy_labels, test_px, tile_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strangeBlock_v4(input_block, nb_layers, filters):\n",
    "    x_list = [input_block]\n",
    "    c_temp = input_block\n",
    "    for i in range(nb_layers):\n",
    "        c_l = LSTM(units=filters, activation='elu', return_sequences=True) (c_temp)\n",
    "        x_list.append(c_l)\n",
    "        merge = Concatenate()(x_list)\n",
    "        c_temp = merge\n",
    "    return c_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MLT_dense(in_shape, in_shape_tile, num_classes):\n",
    "    input_tensor = Input(shape = in_shape, name=\"rnn_input\")\n",
    "    #single pixel model\n",
    "    block = strangeBlock_v4(input_tensor, 5, 12)\n",
    "    block_2 = strangeBlock_v4(block, 3, 20)\n",
    "    final_block = LSTM(64, activation='softmax', return_sequences=False) (block_2)\n",
    "\n",
    "    #cnn model\n",
    "    input_tensor_tile = Input(shape = in_shape_tile, name=\"tile_input\")\n",
    "    cnn_layer1 = ConvLSTM2D(64, kernel_size=2, activation='elu', return_sequences=True)(input_tensor_tile)\n",
    "    # batch norm\n",
    "    cnn_layer2 = ConvLSTM2D(64, kernel_size=2, activation='elu', return_sequences=False) (cnn_layer1)\n",
    "    maxpool = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid') (cnn_layer2)\n",
    "    flatten = Flatten()(maxpool)\n",
    "    concat = concatenate([final_block, flatten])\n",
    "    \n",
    "    landcover = Dense(num_classes,activation='softmax', name='landcover') (concat)\n",
    "    canopy = Dense(1, name='canopy') (concat)\n",
    "\n",
    "    model = Model(inputs=[input_tensor,input_tensor_tile], outputs=[landcover, canopy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_MLT_dense((len(tiles['028012']),7),(len(tiles['028012']),tile_size,tile_size,7), class_count)\n",
    "#model.load_weights('mlt_dense_model.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model('dense_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = 'mlt_dense_model_pcg.hdf5'#your filepath here\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_landcover_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#callbacks_list = [checkpoint]\n",
    "tile_gen = rnn_tiles.rnn_tile_gen(landsat_datasets, lc_labels, canopy_labels, tile_size, class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss={'landcover':'categorical_crossentropy', 'canopy':'mae'}, metrics={'landcover':['accuracy'], 'canopy':['mae']}, loss_weights={\"landcover\":1, \"canopy\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=tile_gen.tile_generator(train_px, batch_size, flatten=True, canopy=True), \n",
    "                    steps_per_epoch=len(train_px) // batch_size // 10, epochs=epochs // 5, verbose=1,\n",
    "                   validation_data=tile_gen.tile_generator(val_px, batch_size, flatten=True, canopy=True),\n",
    "                  validation_steps=len(val_px) // batch_size // 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['landcover_acc'])\n",
    "plt.plot(history.history['val_landcover_acc'])\n",
    "plt.title('Model LC accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['canopy_mean_absolute_error'])\n",
    "plt.plot(history.history['val_canopy_mean_absolute_error'])\n",
    "plt.title('Model canopy error')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.plot(history.history['landcover_loss'])\n",
    "plt.plot(history.history['val_landcover_loss'])\n",
    "plt.plot(history.history['canopy_loss'])\n",
    "plt.plot(history.history['val_canopy_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val', 'Train LC', 'Val LC', 'Train Canopy', 'Val Canopy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(generator = tile_gen.tile_generator(train_px, batch_size=1, flatten=True, canopy=True), steps=len(train_px) // 1, verbose=1)\n",
    "eval_generator = tile_gen.tile_generator(train_px, batch_size=1, flatten=True, canopy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_predictions_train = np.asarray(predictions[0])\n",
    "canopy_pred_train = np.asarray(predictions[1])\n",
    "lc_predictions_train = np.argmax(lc_predictions_train,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_lab_train = np.empty(lc_predictions_train.shape)\n",
    "canopy_true_train = np.empty(canopy_pred_train.shape)\n",
    "count = 0\n",
    "while count < len(lc_predictions_train):\n",
    "        image_b, label_b = next(eval_generator)\n",
    "        #label_b = np.argmax(label_b, axis=-1)\n",
    "        label_lc = np.argmax(label_b['landcover'], axis=-1)\n",
    "        canopy_true_train[count] = label_b['canopy']\n",
    "        lc_lab_train[count] = label_lc\n",
    "        count += 1\n",
    "label_index = lc_lab_train.reshape(len(train_px))\n",
    "pred_index = lc_predictions_train.reshape(len(train_px))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(canopy_true_train, alpha=0.75), plt.hist(canopy_pred_train, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(canopy_true_train, canopy_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "x = (canopy_pred_train * 100).flatten()\n",
    "y = (canopy_true_train * 100).flatten()\n",
    "\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "idx = z.argsort()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x,y,c=z,s=50,edgecolor='')\n",
    "ax.set_xlabel(\"Predictions\")\n",
    "ax.set_ylabel(\"Annotations\")\n",
    "ax.set_title(\"Canopy Cover Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(generator = tile_gen.tile_generator(val_px, batch_size=1, flatten=True, canopy=True), steps=len(test_px) // 1, verbose=1)\n",
    "eval_generator = tile_gen.tile_generator(val_px, batch_size=1, flatten=True, canopy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_predictions = np.asarray(predictions[0])\n",
    "canopy_pred = np.asarray(predictions[1])\n",
    "lc_predictions = np.argmax(lc_predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_lab = np.empty(lc_predictions.shape)\n",
    "canopy_true = np.empty(canopy_pred.shape)\n",
    "count = 0\n",
    "while count < len(lc_predictions):\n",
    "        image_b, label_b = next(eval_generator)\n",
    "        #label_b = np.argmax(label_b, axis=-1)\n",
    "        label_lc = np.argmax(label_b['landcover'], axis=-1)\n",
    "        canopy_true[count] = label_b['canopy']\n",
    "        lc_lab[count] = label_lc\n",
    "        count += 1\n",
    "label_index = lc_lab.reshape(len(test_px)*1*1)\n",
    "pred_index = lc_predictions.reshape(len(test_px)*1*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "# Plot non-normalized confusion matrix\n",
    "util.plot_confusion_matrix(label_index.astype(np.int), pred_index.astype(np.int), classes=np.array(list(class_dict)),\n",
    "                      class_dict=class_dict)\n",
    "# Plot normalized confusion matrix\n",
    "util.plot_confusion_matrix(label_index.astype(np.int), pred_index.astype(np.int), classes=np.array(list(class_dict)),\n",
    "                      class_dict=class_dict,\n",
    "                      normalize=True,\n",
    "                          title=\" \")\n",
    "count = 0\n",
    "for i in range(len(label_index)):\n",
    "    if(label_index[i] == pred_index[i]):\n",
    "        count+=1\n",
    "print(\"Accuracy is {}\".format(count/len(label_index)))\n",
    "\n",
    "plt.savefig(\"confusion_matrix.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "tot = 0\n",
    "for count in range(len(canopy_pred)):\n",
    "    if canopy_true[count] != 0 and canopy_pred[count] !=0:\n",
    "        if canopy_pred[count] < 0:\n",
    "            canopy_pred[count] = 0\n",
    "        total+= np.absolute(canopy_pred[count] - canopy_true[count])\n",
    "        tot+=1\n",
    "print(total/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pred_df = pd.DataFrame({'lc_pred': pred_index,\n",
    "                        'lc_true': label_index,\n",
    "                        'canopy_pred': canopy_pred.flatten(),\n",
    "                        'canopy_true': canopy_true.flatten()})\n",
    "pred_df[pred_df[\"lc_true\"] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=pred_df['canopy_pred'][pred_df['lc_true']!=0], y=pred_df['canopy_true'][pred_df['lc_true']!=0], kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pred_df['canopy_pred'][pred_df['lc_true']!=0]\n",
    "y=pred_df['canopy_true'][pred_df['lc_true']!=0]\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "#ax.set_xlabel(\"Predictions\")\n",
    "#ax.set_ylabel(\"Annotations\")\n",
    "#ax.set_title(\"Canopy Cover Predictions\")\n",
    "\n",
    "g = sns.jointplot(x,y, kind='scatter', alpha=0.2)\n",
    "#g.ax_joint.scatter(x=pred_df['canopy_pred'][pred_df['lc_true']!=0], y=pred_df['canopy_true'][pred_df['lc_true']!=0])\n",
    "g.ax_joint.set_xticks(np.linspace(0,1,6))\n",
    "g.ax_joint.set_xlabel(\"\")\n",
    "g.ax_joint.set_ylabel(\"\")\n",
    "\n",
    "plt.savefig(\"canopy_regression.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(canopy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(canopy_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "#x = (canopy_pred * 100).flatten()\n",
    "#y = (canopy_true * 100).flatten()\n",
    "\n",
    "x=pred_df['canopy_pred'][pred_df['lc_true']!=0]\n",
    "y=pred_df['canopy_true'][pred_df['lc_true']!=0]\n",
    "\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "idx = z.argsort()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "fig,ax = plt.subplots()\n",
    "im = ax.scatter(x,y,c=z,s=50,edgecolor='', alpha=0.7, cmap=\"nipy_spectral\")\n",
    "ax.set_xticks(np.linspace(0,1,6))\n",
    "#ax.set_xlabel(\"Predictions\")\n",
    "#ax.set_ylabel(\"Annotations\")\n",
    "#ax.set_title(\"Canopy Cover Predictions\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.savefig(\"canopy_regression.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_df['canopy_pred'][pred_df['lc_true']!=0])\n",
    "plt.hist(pred_df['canopy_true'][pred_df['lc_true']!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(canopy_pred, canopy_true)\n",
    "plt.xlabel(\"Canopy Fraction Predictions\")\n",
    "plt.ylabel(\"Canopy Fraction Annotations\")\n",
    "plt.savefig(\"canopy_regression.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(canopy_true, canopy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library & dataset\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('iris')\n",
    " \n",
    "\n",
    "sns.jointplot(y=(canopy_true*100).flatten(), x=(canopy_pred*100).flatten(), kind='scatter')\n",
    "sns.jointplot(x=(canopy_true*100).flatten(), y=(canopy_pred*100).flatten(), kind='hex')\n",
    "sns.jointplot(x=(canopy_true*100).flatten(), y=(canopy_pred*100).flatten(), kind='kde')\n",
    "#plt.savefig(\"canopy_reg_dist_cont.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
