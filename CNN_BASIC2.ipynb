{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.plot import adjust_band\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from rasterio.plot import show\n",
    "from itertools import product\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, transform\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset = rasterio.open(\"/deep_data/data/land_cover_data/landcover_reproject.tif\")\n",
    "label_image = label_dataset.read()\n",
    "\n",
    "tiles = [\"028012\", \"028011\"]\n",
    "\n",
    "landsat_dataset = list(i for i in range(len(tiles)))\n",
    "count = 0\n",
    "for tile in tiles:\n",
    "    landsat_dataset[count] = rasterio.open(\"/deep_data/data/combined/combined\" + tile + \".tif\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image projection:\n",
      "PROJCS[\"Albers\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378140,298.2569999999957,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_center\",23],PARAMETER[\"longitude_of_center\",-96],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]\n",
      "Labels projection:\n",
      "EPSG:32618\n"
     ]
    }
   ],
   "source": [
    "# What is the raster's projection?\n",
    "image_proj = landsat_dataset[1].crs # 4326\n",
    "print('Image projection:')\n",
    "print(image_proj)\n",
    "\n",
    "# What is the raster's projection?\n",
    "label_proj = label_dataset.crs\n",
    "print('Labels projection:')\n",
    "print(label_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pixel_locations(image_datasets, train_count, val_count, tile_size):\n",
    "    ### this function pulls out a randomly selected number of pixels from a list of raster datasets\n",
    "    ### and returns a list of training pixel locations and image indices \n",
    "    ### and a list of validation pixel locations and indices\n",
    "    \n",
    "    ## future improvements could make this select classes evenly\n",
    "    train_pixels = []\n",
    "    val_pixels = []\n",
    "    \n",
    "    buffer = math.ceil(tile_size/2)\n",
    "    \n",
    "    total_count = train_count + val_count\n",
    "    for index, image_dataset in enumerate(image_datasets):\n",
    "        #randomly pick `count` num of pixels from each dataset\n",
    "        img_height, img_width = image_dataset.shape\n",
    "        \n",
    "        rows = range(0+buffer, img_height-buffer)\n",
    "        columns = range(0+buffer, img_width-buffer)\n",
    "        #rows_sub, columns_sub = zip(*random.sample(list(zip(rows, columns)), total_count))\n",
    "        \n",
    "        points = random.sample(set(product(rows, columns)), total_count)\n",
    "        \n",
    "        dataset_index = [index] * total_count\n",
    "        dataset_pixels = list(zip(points, dataset_index))\n",
    "        \n",
    "        train_pixels += dataset_pixels[:train_count]\n",
    "        val_pixels += dataset_pixels[train_count:]\n",
    "        \n",
    "        \n",
    "    return (train_pixels, val_pixels)\n",
    "\n",
    "\n",
    "def tile_generator(image_datasets, label_dataset, tile_height, tile_width, pixel_locations, batch_size):\n",
    "    ### this is a keras compatible data generator which generates data and labels on the fly \n",
    "    ### from a set of pixel locations, a list of image datasets, and a label dataset\n",
    "    \n",
    "    # pixel locations looks like [r, c, dataset_index]\n",
    "    label_image = label_dataset.read()\n",
    "    label_image[label_image == 255] = 1\n",
    "\n",
    "    c = r = 0\n",
    "    i = 0\n",
    "    \n",
    "    outProj = Proj(label_dataset.crs)\n",
    "\n",
    "    # assuming all images have the same num of bands\n",
    "    band_count = image_datasets[0].count\n",
    "    class_count = len(np.unique(label_image))\n",
    "    buffer = math.ceil(tile_height / 2)\n",
    "  \n",
    "    while True:\n",
    "        image_batch = np.zeros((batch_size, tile_height, tile_width, band_count-1)) # take one off because we don't want the QA band\n",
    "        label_batch = np.zeros((batch_size, class_count, tile_height, tile_width, 1))\n",
    "        b = 0\n",
    "        while b < batch_size:\n",
    "            # if we're at the end  of the data just restart\n",
    "            if i >= len(pixel_locations):\n",
    "                i=0\n",
    "            c, r = pixel_locations[i][0]\n",
    "            dataset_index = pixel_locations[i][1]\n",
    "            i += 1\n",
    "            \n",
    "            # GET GPS COORDINATES AND USE THESE TO CREATE LABEL TILE\n",
    "            (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "            inProj = Proj(image_datasets[dataset_index].crs)\n",
    "            if inProj != outProj:\n",
    "                x,y = transform(inProj,outProj,x,y)\n",
    "            #    \n",
    "            \n",
    "        \n",
    "                \n",
    "            tile = image_datasets[dataset_index].read(list(np.arange(1, band_count+1)), window=Window(c-buffer, r-buffer, tile_width, tile_height))\n",
    "            Labeltile = label_dataset.read(0, window=Window(x-buffer, y-buffer, tile_width, tile_height))\n",
    "\n",
    "            if np.amax(tile) == 0: # don't include if it is part of the image with no pixels\n",
    "                pass\n",
    "            elif np.isnan(tile).any() == True or -9999 in tile: \n",
    "                # we don't want tiles containing nan or -999 this comes from edges\n",
    "                # this also takes a while and is inefficient\n",
    "                pass\n",
    "            elif tile.shape != (band_count, tile_width, tile_height):\n",
    "                print('wrong shape')\n",
    "                print(tile.shape)\n",
    "                # somehow we're randomly getting tiles without the correct dimensions\n",
    "                pass\n",
    "            elif np.isin(tile[7,:,:], [352, 368, 392, 416, 432, 480, 840, 864, 880, 904, 928, 944, 1352]).any() == True:\n",
    "                # make sure pixel doesn't contain clouds\n",
    "                # this is probably pretty inefficient but only checking width x height for each tile\n",
    "                # read more here: https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1873_US_Landsat_ARD_DFCB_0.pdf\n",
    "                #print('Found some cloud.')\n",
    "                #print(tile[7,:,:])\n",
    "                pass\n",
    "            else:\n",
    "                tile = adjust_band(tile[0:7])\n",
    "                # reshape from raster format to image format\n",
    "                reshaped_tile = reshape_as_image(tile)\n",
    "\n",
    "                # find gps of that pixel within the image\n",
    "                (x, y) = image_datasets[dataset_index].xy(r, c)\n",
    "\n",
    "                # convert the point we're sampling from to the same projection as the label dataset if necessary\n",
    "                inProj = Proj(image_datasets[dataset_index].crs)\n",
    "                if inProj != outProj:\n",
    "                    x,y = transform(inProj,outProj,x,y)\n",
    "\n",
    "                # reference gps in label_image\n",
    "                row, col = label_dataset.index(x,y)\n",
    "\n",
    "                # find label\n",
    "                label = label_image[:, row, col]\n",
    "                # if this label is part of the unclassified area then ignore\n",
    "                if label == 0 or np.isnan(label).any() == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    # add label to the batch in a one hot encoding style\n",
    "                    label_batch[b][label] = 1\n",
    "                    image_batch[b] = reshaped_tile\n",
    "                    b += 1\n",
    "        yield (image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 11\n",
    "batch_size = 25\n",
    "label_image[label_image == 255] = 1\n",
    "epochs = 100\n",
    "classNum = len(np.unique(label_image))\n",
    "(train_pixels, val_pixels) = make_pixel_locations(landsat_dataset, 50000, 10000, tile_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pixels = val_pixels[:math.ceil(len(val_pixels)/3)]\n",
    "val_pixels = val_pixels[math.ceil(len(val_pixels)/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 11, 7)\n",
      "100000\n",
      "13333\n",
      "6667\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = tile_size, tile_size\n",
    "img_bands = landsat_dataset[0].count\n",
    "\n",
    "input_shape = (img_rows, img_cols, img_bands)\n",
    "print(input_shape)\n",
    "\n",
    "print(len(train_pixels))\n",
    "print(len(val_pixels))\n",
    "print(len(test_pixels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 100)         6400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 2, 128)         115328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 23)                11799     \n",
      "=================================================================\n",
      "Total params: 199,575\n",
      "Trainable params: 199,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(100, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(classNum, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics=['accuracy']\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 100s 25ms/step - loss: 1.6693 - acc: 0.4712 - val_loss: 1.5457 - val_acc: 0.5140\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 93s 23ms/step - loss: 1.5901 - acc: 0.4897 - val_loss: 1.4939 - val_acc: 0.5220\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 1.5217 - acc: 0.5117 - val_loss: 1.8562 - val_acc: 0.3622\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 1.5029 - acc: 0.5163 - val_loss: 1.4635 - val_acc: 0.5355\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 93s 23ms/step - loss: 1.5064 - acc: 0.5114 - val_loss: 1.5018 - val_acc: 0.5287\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 1.4810 - acc: 0.5184 - val_loss: 1.6410 - val_acc: 0.5051\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 1.4253 - acc: 0.5387 - val_loss: 1.4868 - val_acc: 0.5294\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 92s 23ms/step - loss: 1.4496 - acc: 0.5257 - val_loss: 1.4623 - val_acc: 0.5336\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 92s 23ms/step - loss: 1.4320 - acc: 0.5293 - val_loss: 1.4661 - val_acc: 0.5345\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 1.3736 - acc: 0.5517 - val_loss: 1.6536 - val_acc: 0.4684\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 1.3703 - acc: 0.5493 - val_loss: 1.4767 - val_acc: 0.5394\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 93s 23ms/step - loss: 1.3643 - acc: 0.5476 - val_loss: 1.6097 - val_acc: 0.5183\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 1.3296 - acc: 0.5612 - val_loss: 1.6795 - val_acc: 0.4890\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 1.2804 - acc: 0.5773 - val_loss: 1.6729 - val_acc: 0.5102\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 1.2903 - acc: 0.5714 - val_loss: 1.5859 - val_acc: 0.5127\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 1.2636 - acc: 0.5765 - val_loss: 1.7354 - val_acc: 0.5012\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 1.2101 - acc: 0.5970 - val_loss: 1.9780 - val_acc: 0.4149\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 1.2042 - acc: 0.5954 - val_loss: 1.7404 - val_acc: 0.5171\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 1.1825 - acc: 0.6012 - val_loss: 1.8979 - val_acc: 0.4958\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 1.1584 - acc: 0.6082 - val_loss: 2.1556 - val_acc: 0.4383\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 1.1287 - acc: 0.6185 - val_loss: 2.1328 - val_acc: 0.4721\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 1.1251 - acc: 0.6181 - val_loss: 1.9012 - val_acc: 0.4933\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 1.1148 - acc: 0.6190 - val_loss: 1.9858 - val_acc: 0.4884\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 1.0690 - acc: 0.6361 - val_loss: 2.6694 - val_acc: 0.4019\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 1.0806 - acc: 0.6318 - val_loss: 2.0510 - val_acc: 0.4786\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 1.0758 - acc: 0.6335 - val_loss: 2.2570 - val_acc: 0.4780\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 1.0405 - acc: 0.6442 - val_loss: 2.6077 - val_acc: 0.3940\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 1.0307 - acc: 0.6481 - val_loss: 2.3993 - val_acc: 0.4776\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 1.0226 - acc: 0.6514 - val_loss: 2.2236 - val_acc: 0.4765\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 1.0246 - acc: 0.6493 - val_loss: 2.7003 - val_acc: 0.3829\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 81s 20ms/step - loss: 0.9816 - acc: 0.6656 - val_loss: 2.3066 - val_acc: 0.5016\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.9940 - acc: 0.6609 - val_loss: 2.4577 - val_acc: 0.4557\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.9855 - acc: 0.6624 - val_loss: 2.5178 - val_acc: 0.4864\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 0.9643 - acc: 0.6722 - val_loss: 2.8152 - val_acc: 0.3584\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.9735 - acc: 0.6674 - val_loss: 2.4924 - val_acc: 0.4608\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.9709 - acc: 0.6693 - val_loss: 2.3823 - val_acc: 0.4793\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.9422 - acc: 0.6787 - val_loss: 3.2623 - val_acc: 0.4030\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 81s 20ms/step - loss: 0.9405 - acc: 0.6815 - val_loss: 2.6002 - val_acc: 0.4750\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.9428 - acc: 0.6790 - val_loss: 2.5057 - val_acc: 0.4717\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.9259 - acc: 0.6852 - val_loss: 2.4615 - val_acc: 0.4844\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 82s 20ms/step - loss: 0.9169 - acc: 0.6897 - val_loss: 2.8877 - val_acc: 0.4276\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.9236 - acc: 0.6869 - val_loss: 2.6741 - val_acc: 0.4780\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.9201 - acc: 0.6877 - val_loss: 2.5779 - val_acc: 0.4856\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8977 - acc: 0.6951 - val_loss: 3.2335 - val_acc: 0.4050\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.9001 - acc: 0.6966 - val_loss: 3.1752 - val_acc: 0.4618\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.9060 - acc: 0.6943 - val_loss: 2.8523 - val_acc: 0.4576\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.8742 - acc: 0.7018 - val_loss: 2.8154 - val_acc: 0.4647\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 82s 20ms/step - loss: 0.8602 - acc: 0.7091 - val_loss: 3.3060 - val_acc: 0.4282\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.8799 - acc: 0.7029 - val_loss: 2.9738 - val_acc: 0.4711\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8696 - acc: 0.7066 - val_loss: 2.7827 - val_acc: 0.4777\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.8588 - acc: 0.7097 - val_loss: 3.2738 - val_acc: 0.3991\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 82s 20ms/step - loss: 0.8570 - acc: 0.7114 - val_loss: 3.2074 - val_acc: 0.4775\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8542 - acc: 0.7129 - val_loss: 3.1640 - val_acc: 0.4660\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.8745 - acc: 0.7056 - val_loss: 3.1916 - val_acc: 0.4442\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 79s 20ms/step - loss: 0.8400 - acc: 0.7184 - val_loss: 3.4623 - val_acc: 0.4429\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.8627 - acc: 0.7109 - val_loss: 2.9829 - val_acc: 0.4687\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8414 - acc: 0.7168 - val_loss: 3.4623 - val_acc: 0.4651\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.8458 - acc: 0.7179 - val_loss: 3.6962 - val_acc: 0.3982\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.8501 - acc: 0.7164 - val_loss: 3.0508 - val_acc: 0.4687\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8490 - acc: 0.7168 - val_loss: 3.1623 - val_acc: 0.4611\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.8343 - acc: 0.7196 - val_loss: 3.8348 - val_acc: 0.4106\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 79s 20ms/step - loss: 0.8026 - acc: 0.7302 - val_loss: 3.5149 - val_acc: 0.4393\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.8082 - acc: 0.7286 - val_loss: 3.0711 - val_acc: 0.4820\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8258 - acc: 0.7242 - val_loss: 3.2774 - val_acc: 0.4699\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 81s 20ms/step - loss: 0.8332 - acc: 0.7253 - val_loss: 3.6992 - val_acc: 0.4308\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.8174 - acc: 0.7281 - val_loss: 3.3721 - val_acc: 0.4783\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8249 - acc: 0.7256 - val_loss: 3.2388 - val_acc: 0.4556\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.8217 - acc: 0.7273 - val_loss: 3.9053 - val_acc: 0.3977\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 80s 20ms/step - loss: 0.8147 - acc: 0.7305 - val_loss: 3.2110 - val_acc: 0.4585\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8340 - acc: 0.7239 - val_loss: 3.3951 - val_acc: 0.4749\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.8198 - acc: 0.7266 - val_loss: 3.3056 - val_acc: 0.4648\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 80s 20ms/step - loss: 0.7786 - acc: 0.7409 - val_loss: 3.6393 - val_acc: 0.4441\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 0.8171 - acc: 0.7303 - val_loss: 3.4389 - val_acc: 0.4591\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.8414 - acc: 0.7221 - val_loss: 3.3971 - val_acc: 0.4600\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 0.7840 - acc: 0.7399 - val_loss: 4.2407 - val_acc: 0.3834\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 81s 20ms/step - loss: 0.7908 - acc: 0.7394 - val_loss: 3.5768 - val_acc: 0.4657\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.8233 - acc: 0.7314 - val_loss: 3.5444 - val_acc: 0.4702\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.8102 - acc: 0.7339 - val_loss: 3.1442 - val_acc: 0.4676\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 79s 20ms/step - loss: 0.7699 - acc: 0.7467 - val_loss: 4.3494 - val_acc: 0.4222\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.8190 - acc: 0.7331 - val_loss: 3.3606 - val_acc: 0.4519\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.8090 - acc: 0.7339 - val_loss: 3.4438 - val_acc: 0.4801\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.7840 - acc: 0.7411 - val_loss: 4.2615 - val_acc: 0.4023\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 81s 20ms/step - loss: 0.8015 - acc: 0.7382 - val_loss: 3.7389 - val_acc: 0.4591\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.7998 - acc: 0.7378 - val_loss: 3.4574 - val_acc: 0.4603\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.7733 - acc: 0.7466 - val_loss: 4.2906 - val_acc: 0.4029\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 78s 20ms/step - loss: 0.7819 - acc: 0.7463 - val_loss: 3.8246 - val_acc: 0.4437\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.7708 - acc: 0.7474 - val_loss: 3.8614 - val_acc: 0.4587\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.7858 - acc: 0.7432 - val_loss: 3.7913 - val_acc: 0.4758\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.7822 - acc: 0.7462 - val_loss: 4.0808 - val_acc: 0.3823\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 82s 20ms/step - loss: 0.7702 - acc: 0.7496 - val_loss: 3.5554 - val_acc: 0.4701\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.8008 - acc: 0.7425 - val_loss: 3.5607 - val_acc: 0.4766\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.7760 - acc: 0.7467 - val_loss: 4.1361 - val_acc: 0.4043\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 78s 20ms/step - loss: 0.7691 - acc: 0.7493 - val_loss: 3.9081 - val_acc: 0.4628\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.7577 - acc: 0.7521 - val_loss: 3.7505 - val_acc: 0.4618\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.7858 - acc: 0.7435 - val_loss: 3.6132 - val_acc: 0.4705\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 81s 20ms/step - loss: 0.7662 - acc: 0.7530 - val_loss: 4.0533 - val_acc: 0.4311\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.7431 - acc: 0.7580 - val_loss: 3.6284 - val_acc: 0.4715\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.7755 - acc: 0.7485 - val_loss: 3.7583 - val_acc: 0.4654\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.7701 - acc: 0.7518 - val_loss: 4.3297 - val_acc: 0.3798\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 80s 20ms/step - loss: 0.7339 - acc: 0.7607 - val_loss: 3.6882 - val_acc: 0.4651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f9cec7f28>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=tile_generator(landsat_dataset, label_dataset, tile_size, tile_size, train_pixels, batch_size), \n",
    "                    steps_per_epoch=len(train_pixels) // batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=tile_generator(landsat_dataset, label_dataset, tile_size, tile_size, val_pixels, batch_size),\n",
    "                    validation_steps=len(val_pixels) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - 13s 51ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.111856063505761, 0.3353383461279528]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator=tile_generator(landsat_dataset, label_dataset, tile_size, tile_size, test_pixels, batch_size), \n",
    "                        steps=len(test_pixels) // batch_size,\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
